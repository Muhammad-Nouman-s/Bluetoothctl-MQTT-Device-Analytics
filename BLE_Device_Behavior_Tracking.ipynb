{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9785381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "\"\"\"\n",
    "This block prepares the environment and parsing tools used throughout the BLE dataset analysis.\n",
    "It imports the required Python libraries for file handling, data processing, and visualization,\n",
    "sets paths to the MQTT CSV logs, and defines the BLE gateway MAC address. It also pre-compiles\n",
    "regular expressions for extracting MAC addresses, 128-bit UUIDs, and hexadecimal fields from\n",
    "the BLE logs. Finally, it configures a logging system to produce structured, timestamped\n",
    "diagnostic messages.\n",
    "\"\"\"\n",
    "# ============================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import json\n",
    "import logging                 \n",
    "import re                      # regex for MAC / UUID extraction\n",
    "from collections import Counter\n",
    "from typing import Any, List, Tuple, Optional, Sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Data paths\n",
    "# -----------------------------\n",
    "\n",
    "# Dataset path (update as needed)\n",
    "DEFAULT_CSV: Path = Path(\"data\") / \"mqtt_input.csv\"\n",
    "\n",
    "# Gateway MAC (update as needed)\n",
    "GATEWAY_MAC: str = \"AA:BB:CC:DD:EE:FF\"\n",
    "GATEWAY_MAC = GATEWAY_MAC.upper()\n",
    "\n",
    "\n",
    "# -----------------\n",
    "# Regex definitions\n",
    "# -----------------\n",
    "\n",
    "# Generic MAC address pattern (AA:BB:CC:DD:EE:FF)\n",
    "# Matches any MAC address inside BLE log lines.\n",
    "MAC_RE = re.compile(r\"([0-9A-Fa-f]{2}(?::[0-9A-Fa-f]{2}){5})\")\n",
    "\n",
    "# 128-bit UUID format used in BLE, 8-4-4-4-12 hex\n",
    "UUID128_RE = re.compile(\n",
    "    r\"([0-9A-Fa-f]{8}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{12})\"\n",
    ")\n",
    "\n",
    "# Single hexadecimal byte (00–FF)\n",
    "# Used to validate TX Power, flags, and other 1-byte fields.\n",
    "HEX_BYTE_RE = re.compile(r\"^[0-9A-Fa-f]{2}$\")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Logging configuration\n",
    "# -----------------------\n",
    "\n",
    "# INFO-level logging with timestamps for consistent diagnostic output.\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(asctime)s] [%(levelname)s] %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9050b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "This block loads the MQTT CSV file and converts the nested JSON payloads into a structured form\n",
    "suitable for BLE analysis. The `load_csv()` function reads the file and reports how many MQTT \n",
    "events (rows) were recorded. The `safe_json_load()` function safely parses each payload string \n",
    "into a dictionary, ensuring that malformed JSON does not interrupt execution. The `parse_payload()` \n",
    "function applies this parser to every row and extracts two key fields: the internal event timestamp\n",
    "(`log_timestamp`) and the raw bluetoothctl scan output (`revelations_raw`), which contains MAC addresses,\n",
    "RSSI values, and other BLE data. \n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def load_csv(file_path: str) -> pd.DataFrame:\n",
    "    \n",
    "    logging.info(f\"Loading CSV file: {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    logging.info(f\"Loaded {len(df)} rows and columns: {df.columns.tolist()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def safe_json_load(text: Any) -> dict:\n",
    "    if not isinstance(text, str):\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        logging.debug(f\"Failed to parse JSON: {text}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def parse_payload(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"payload\" not in df.columns:\n",
    "        raise KeyError(\"CSV does not contain required 'payload' column.\")\n",
    "\n",
    "    # Parse JSON payloads\n",
    "    df[\"payload_json\"] = df[\"payload\"].apply(safe_json_load)\n",
    "\n",
    "    # Extract relevant fields from JSON payload\n",
    "    df[\"log_timestamp\"] = df[\"payload_json\"].apply(lambda x: x.get(\"timestamp\"))\n",
    "    df[\"revelations_raw\"] = df[\"payload_json\"].apply(lambda x: x.get(\"revelations\"))\n",
    "\n",
    "    logging.info(\"Parsed JSON payloads: extracted 'log_timestamp' and 'revelations_raw'.\")\n",
    "    logging.info(f\"Unique payload timestamps: {df['log_timestamp'].nunique()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load CSV\n",
    "df = load_csv(DEFAULT_CSV)\n",
    "\n",
    "# Parse JSON payload inside it\n",
    "df = parse_payload(df)\n",
    "\n",
    "# Show the first few entries of timestamps (for validation)\n",
    "print(\"\\n--- SAMPLE TIMESTAMPS FROM PAYLOAD JSON ---\")\n",
    "print(df[\"log_timestamp\"].head().to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a1690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69216de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "This block inspects the raw MQTT metadata before further processing. It examines the `timestamp_client`\n",
    "column to understand how MQTT message timing is recorded, checks the `topic` column to identify which \n",
    "MQTT topics are used, and inspects the `payload` column to view the raw JSON strings containing the BLE\n",
    "scan data. This step clarifies the structure and content of the incoming MQTT messages and ensures that\n",
    "the timestamp, topic, and payload formats are understood before parsing them in later stages.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Inspect timestamp_client\n",
    "# -------------------------------\n",
    "print(\" TIMESTAMP_CLIENT (from MQTT client)\")\n",
    "\n",
    "ts_utc = pd.to_datetime(df[\"timestamp_client\"], utc=True, errors=\"coerce\")\n",
    "ts_rome = ts_utc.dt.tz_convert(\"Europe/Rome\")\n",
    "\n",
    "print(\"Type (UTC parsed):\", ts_utc.dtype)\n",
    "print(\"Unique values:\", ts_utc.nunique(dropna=True))\n",
    "\n",
    "print(\"\\nSample values:\")\n",
    "print(ts_rome.head(10).to_string(index=False))\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Inspect topic column\n",
    "# -------------------------------\n",
    "print(\"\\n\\n TOPIC Column (MQTT topic names)\")\n",
    "print(\"Unique topics:\", df[\"topic\"].nunique())\n",
    "\n",
    "print(\"\\nList of all distinct topics:\")\n",
    "for t in df[\"topic\"].unique():\n",
    "    print(\" -\", t)\n",
    "\n",
    "print(\"\\nSample topic values:\")\n",
    "print(df[\"topic\"].head(3).to_string(index=False))\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Inspect payload column (raw JSON)\n",
    "# -------------------------------\n",
    "print(\"\\n\\n PAYLOAD Column (raw JSON strings)\")\n",
    "print(\"Showing first 5 payload entries:\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"[PAYLOAD {i}]\")\n",
    "    print(df[\"payload\"].iloc[i])\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8900e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#   Remove ANSI Codes (BLE-safe)\n",
    "# Purpose:\n",
    "#   - bluetoothctl adds ANSI color / cursor control codes\n",
    "#   - These break regex parsing\n",
    "#   - IMPORTANT:\n",
    "#       • Do NOT remove raw control bytes (e.g. \\x02)\n",
    "#       • They may be part of real BLE payloads (iBeacon = 02 15)\n",
    "# ============================================================\n",
    "\n",
    "def clean_ansi(text: Any) -> Any:\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Remove ANSI escape sequences only\n",
    "    text = re.sub(r\"\\x1B\\[[0-?]*[ -/]*[@-~]\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Apply cleaning to revelations_raw\n",
    "df[\"revelations_clean\"] = df[\"revelations_raw\"].apply(clean_ansi)\n",
    "\n",
    "logging.info(\"Created 'revelations_clean' by removing ANSI escape sequences only.\")\n",
    "\n",
    "# Optional verification output\n",
    "print(\"\\n===================================================\")\n",
    "print(\"   SAMPLE REVELATIONS (ANSI CLEANED)\")\n",
    "print(\"===================================================\\n\")\n",
    "\n",
    "for i in range(min(2, len(df))):\n",
    "    print(f\"--- ROW {i} ---\")\n",
    "    print(df[\"revelations_raw\"].iloc[i])\n",
    "    print(f\"--- ROW {i} ---\")\n",
    "    print(df[\"revelations_clean\"].iloc[i])\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c473d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Matches bracketed device event tags ([NEW], [CHG], [DEL]) even when they contain embedded ASCII control characters (0x00–0x1F)\n",
    "or irregular whitespace, captures the valid event token, and normalizes malformed tags to their canonical form while preserving \n",
    "non-string inputs unchanged.\n",
    "\"\"\"\n",
    "\n",
    "CONTROL_TAG_RE = re.compile(r\"\\[\\s*[\\x00-\\x1F]*\\s*(NEW|CHG|DEL)\\s*[\\x00-\\x1F]*\\s*\\]\")\n",
    "\n",
    "def normalize_device_tags(text: Any) -> Any:\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Normalize [\\x01\\x02CHG\\x01\\x02] → [CHG]\n",
    "    return CONTROL_TAG_RE.sub(r\"[\\1]\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac202e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"revelations_clean\"] = (\n",
    "    df[\"revelations_raw\"]\n",
    "      .apply(clean_ansi)\n",
    "      .apply(normalize_device_tags)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10abe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "This block analyzes controller-level activity in the Bluetooth logs by scanning the `revelations_clean`\n",
    "column for patterns related to the BLE controller—such as `[CHG] Controller`, `Discovering: yes/no`, \n",
    "`Powered: yes/no`, and `Pairable: yes/no`. It counts how often each controller state occurs and determines \n",
    "how many MQTT messages contain any controller-related event. This helps describe the behaviour of the BLE \n",
    "scanner itself so that later analysis can clearly separate controller state changes from actual device-level \n",
    "events.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "controller_patterns = {\n",
    "    \"[CHG] Controller\": 0,\n",
    "    \"[NEW] Controller\": 0,\n",
    "    \"[DEL] Controller\": 0,\n",
    "    \"Discovering: yes\": 0,\n",
    "    \"Discovering: no\": 0,\n",
    "    \"Powered: yes\": 0,\n",
    "    \"Powered: no\": 0,\n",
    "    \"Pairable: yes\": 0,\n",
    "    \"Pairable: no\": 0,\n",
    "}\n",
    "\n",
    "total_rows = len(df)\n",
    "\n",
    "for text in df[\"revelations_clean\"]: #text = one block of text for one timestamp.\n",
    "    if not isinstance(text, str):\n",
    "        continue\n",
    "\n",
    "    for pattern in controller_patterns.keys():\n",
    "        if pattern in text:\n",
    "            controller_patterns[pattern] += 1\n",
    "\n",
    "# Count how many rows contain ANY controller keyword\n",
    "def has_any_controller_event(text: str) -> bool:\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    if \"Controller \" in text:\n",
    "        return True\n",
    "    # also consider lines that only say \"Discovering: ...\" or \"Powered: ...\"\n",
    "    for key in [\"Discovering:\", \"Powered:\", \"Pairable:\"]:\n",
    "        if key in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "rows_with_controller = sum(\n",
    "    1 for txt in df[\"revelations_clean\"] if has_any_controller_event(txt)\n",
    ")\n",
    "\n",
    "print(\"\\n===================================================\")\n",
    "print(\"   CONTROLLER-LEVEL EVENTS \")\n",
    "print(\"===================================================\\n\")\n",
    "\n",
    "print(f\"Total MQTT messages (rows): {total_rows}\")\n",
    "# print(f\"Rows containing at least one controller-related event: {rows_with_controller}\")\n",
    "print()\n",
    "\n",
    "print(\"Event counts (controller-level):\")\n",
    "for pattern, count in controller_patterns.items():\n",
    "    print(f\"  {pattern:<18} --> {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "This block scans `revelations_clean` for `[NEW]`, `[CHG]`, and `[DEL]` device events, counts their\n",
    "occurrences, and identifies how many MQTT messages contain device activity. `[NEW]` marks first \n",
    "detection, `[CHG]` updates an existing device, and `[DEL]` indicates removal or timeout. These events \n",
    "are later used to compute device entry/exit times and presence duration.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "device_new_count = 0\n",
    "device_chg_count = 0\n",
    "device_del_count = 0\n",
    "\n",
    "rows_with_device_events = 0\n",
    "\n",
    "for text in df[\"revelations_clean\"]:\n",
    "    if not isinstance(text, str):\n",
    "        continue\n",
    "\n",
    "    has_device = False\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        line_stripped = line.strip()\n",
    "        if line_stripped.startswith(\"[NEW] Device \"):\n",
    "            device_new_count += 1\n",
    "            has_device = True\n",
    "        elif line_stripped.startswith(\"[CHG] Device \"):\n",
    "            device_chg_count += 1\n",
    "            has_device = True\n",
    "        elif line_stripped.startswith(\"[DEL] Device \"):\n",
    "            device_del_count += 1\n",
    "            has_device = True\n",
    "\n",
    "    if has_device:\n",
    "        rows_with_device_events += 1\n",
    "\n",
    "total_device_events = device_new_count + device_chg_count + device_del_count\n",
    "\n",
    "print(\"\\n===================================================\")\n",
    "print(\"   DEVICE-LEVEL EVENTS  \")\n",
    "print(\"===================================================\\n\")\n",
    "\n",
    "print(f\"Rows containing at least one DEVICE event: {rows_with_device_events}\")\n",
    "print(f\"Total DEVICE events (NEW + CHG + DEL):    {total_device_events}\\n\")\n",
    "\n",
    "print(\"DEVICE-LEVEL EVENTS  type:\")\n",
    "print(f\"  [NEW] Device  --> {device_new_count} events\")\n",
    "print(f\"  [CHG] Device  --> {device_chg_count} events\")\n",
    "print(f\"  [DEL] Device  --> {device_del_count} events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5afeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "This block extracts all MAC addresses from the cleaned Bluetooth logs, flattens them into a single list, \n",
    "and counts how many times each MAC appears. It separates the gateway MAC from real BLE devices and produces \n",
    "a summary of how many unique devices were detected and how frequently each one appeared in the logs. \n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "#\n",
    "# -----------------------------\n",
    "# 1. Extract MAC list per row\n",
    "# -----------------------------\n",
    "def extract_mac_list(block_text: Any) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extract ALL MAC addresses appearing in a bluetoothctl block.\n",
    "    \n",
    "    This includes:\n",
    "      - Device MACs (BLE devices)\n",
    "      - Controller MAC (gateway)\n",
    "    \n",
    "    Filtering of gateway MAC will happen later.\n",
    "    \"\"\"\n",
    "    if not isinstance(block_text, str):\n",
    "        return []\n",
    "    return [m.upper() for m in MAC_RE.findall(block_text)]\n",
    "\n",
    "\n",
    "df[\"mac_list\"] = df[\"revelations_clean\"].apply(extract_mac_list)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Flatten into a single list\n",
    "# -----------------------------\n",
    "all_macs = []\n",
    "for macs in df[\"mac_list\"]:\n",
    "    if isinstance(macs, list):\n",
    "        all_macs.extend(macs)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Unique devices\n",
    "# -----------------------------\n",
    "unique_macs = sorted(set(all_macs))\n",
    "total_unique = len(unique_macs)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Event counts per MAC\n",
    "# -----------------------------\n",
    "from collections import Counter\n",
    "mac_event_counts = Counter(all_macs)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Identify gateway MAC\n",
    "# -----------------------------\n",
    "GATEWAY_MAC = \"0C:9A:42:18:A5:A4\"\n",
    "\n",
    "gateway_event_count = mac_event_counts.get(GATEWAY_MAC, 0)\n",
    "\n",
    "# Devices only = remove gateway\n",
    "device_macs = [m for m in unique_macs if m != GATEWAY_MAC]\n",
    "total_device_count = len(device_macs)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6. PRINT SUMMARY\n",
    "# -----------------------------\n",
    "print(\"\\n===================================================\")\n",
    "print(\"      BLE DEVICE IDENTIFICATION \")\n",
    "print(\"===================================================\\n\")\n",
    "\n",
    "print(f\"Total MAC addresses detected : {total_unique}\")\n",
    "print(f\"Total devices (excluding gateway): {total_device_count}\")\n",
    "print(f\"Gateway MAC ({GATEWAY_MAC}) detected with: {gateway_event_count} events\\n\")\n",
    "\n",
    "print(\"\\n===================================================\")\n",
    "print(\"     LIST OF DEVICES (MAC → Event Count)\")\n",
    "print(\"===================================================\\n\")\n",
    "\n",
    "for mac, cnt in mac_event_counts.most_common():\n",
    "    print(f\"{mac:<18} ---------> {cnt} events\")\n",
    "\n",
    "print(\"\\nTotal MACs printed:\", len(mac_event_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a444bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "\"\"\"\n",
    "Ensures a UTC timestamp column (df[\"ts_utc\"]), extracts [NEW]/[CHG]/[DEL] device events with their MAC addresses, \n",
    "groups events by MAC into a dictionary of {timestamp, event_type, line}, sorts each MAC’s events,\n",
    "and prints the complete per-device event history.\n",
    "\"\"\"\n",
    "# ============================================================\n",
    "\n",
    "# Ensure a timestamp column exists (use ts_utc if already created; else create it)\n",
    "if \"ts_utc\" not in df.columns:\n",
    "    df = df.copy()\n",
    "    df[\"ts_utc\"] = pd.to_datetime(df[\"timestamp_client\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "mac_events_dict = {}  # MAC -> list of dicts {ts, event_type, line}\n",
    "\n",
    "for ts, text in zip(df[\"ts_utc\"].tolist(), df[\"revelations_clean\"].tolist()):\n",
    "    if not isinstance(text, str):\n",
    "        continue\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "\n",
    "        if line.startswith(\"[NEW] Device \"):\n",
    "            event_type = \"NEW\"\n",
    "        elif line.startswith(\"[CHG] Device \"):\n",
    "            event_type = \"CHG\"\n",
    "        elif line.startswith(\"[DEL] Device \"):\n",
    "            event_type = \"DEL\"\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        m = MAC_RE.search(line)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        mac = m.group(1).upper()\n",
    "\n",
    "        if mac not in mac_events_dict:\n",
    "            mac_events_dict[mac] = []\n",
    "\n",
    "        mac_events_dict[mac].append({\n",
    "            \"timestamp\": ts,\n",
    "            \"event_type\": event_type,\n",
    "            \"line\": line,\n",
    "        })\n",
    "\n",
    "# Sort events per MAC by timestamp\n",
    "for mac in mac_events_dict:\n",
    "    mac_events_dict[mac] = sorted(\n",
    "        mac_events_dict[mac],\n",
    "        key=lambda x: (pd.Timestamp.min if pd.isna(x[\"timestamp\"]) else x[\"timestamp\"])\n",
    "    )\n",
    "\n",
    "print(\"\\n===================================================\")\n",
    "print(\"   ALL DEVICE EVENTS GROUPED BY MAC (WITH TIMESTAMP)\")\n",
    "print(\"===================================================\\n\")\n",
    "\n",
    "for mac, events in mac_events_dict.items():\n",
    "    print(f\"\\n▶ MAC: {mac}   (Total events: {len(events)})\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for e in events:\n",
    "        ts = e.get(\"timestamp\")\n",
    "        ts_str = str(ts) if pd.notna(ts) else \"NaT\"\n",
    "\n",
    "        line = e.get(\"line\", \"\")\n",
    "        print(f\"{ts_str} | {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ece7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "\"\"\"\n",
    "Eextract iBeacon advertisements by reconstructing ManufacturerData payload bytes, detecting valid iBeacon signatures, \n",
    "and decoding UUID,  Major, and Minor values; associates each decoded beacon with the current MAC address, returns (mac, uuid, major, minor) \n",
    "tuples per block, flattens and deduplicates them into mac_beacon_df based on full tuple uniqueness, and prints the total number of \n",
    "unique iBeacon identities and their details.\n",
    "\"\"\"\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "from typing import Any, List, Tuple, Optional\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Regex helpers\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "HEX_IN_LINE_RE = re.compile(r\"(?:0x)?([0-9A-Fa-f]{2})\")\n",
    "\n",
    "IBEACON_PATTERNS = [\n",
    "    bytes.fromhex(\"4c 00 02 15\"),\n",
    "    bytes.fromhex(\"ff 4c 00 02 15\"),\n",
    "    bytes.fromhex(\"02 15\"),\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper functions\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def uuid_from_bytes(b: bytes) -> str:\n",
    "    h = b.hex()\n",
    "    return f\"{h[0:8]}-{h[8:12]}-{h[12:16]}-{h[16:20]}-{h[20:32]}\"\n",
    "\n",
    "def parse_ibeacon_bytes(payload: bytes) -> Optional[Tuple[str, int, int]]:\n",
    "    for pat in IBEACON_PATTERNS:\n",
    "        idx = payload.find(pat)\n",
    "        if idx == -1:\n",
    "            continue\n",
    "\n",
    "        start = idx + len(pat)\n",
    "        if len(payload) < start + 16 + 2 + 2:\n",
    "            continue\n",
    "\n",
    "        uuid_b  = payload[start : start + 16]\n",
    "        major_b = payload[start + 16 : start + 18]\n",
    "        minor_b = payload[start + 18 : start + 20]\n",
    "\n",
    "        return (\n",
    "            uuid_from_bytes(uuid_b),\n",
    "            int.from_bytes(major_b, \"big\"),\n",
    "            int.from_bytes(minor_b, \"big\"),\n",
    "        )\n",
    "    return None\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Main extractor\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def extract_ibeacon_tuples(block_text: Any) -> List[Tuple[str, str, int, int]]:\n",
    "    \"\"\"\n",
    "    Returns a list of (mac, uuid, major, minor) extracted from one revelations text block.\n",
    "    \"\"\"\n",
    "    results: List[Tuple[str, str, int, int]] = []\n",
    "    if not isinstance(block_text, str):\n",
    "        return results\n",
    "\n",
    "    current_mac: Optional[str] = None\n",
    "    collecting = False\n",
    "    collected = bytearray()\n",
    "\n",
    "    for line in block_text.splitlines():\n",
    "        line = line.rstrip()\n",
    "\n",
    "        # Track current MAC\n",
    "        m = MAC_RE.search(line)\n",
    "        if m:\n",
    "            current_mac = m.group(1).upper()\n",
    "\n",
    "        # Start ManufacturerData block\n",
    "        if \"ManufacturerData\" in line and current_mac:\n",
    "            collecting = True\n",
    "            collected.clear()\n",
    "            continue\n",
    "\n",
    "        # End block on next event boundary\n",
    "        if collecting and (line.startswith(\"[\") or \"Device \" in line):\n",
    "            parsed = parse_ibeacon_bytes(bytes(collected))\n",
    "            if parsed and current_mac:\n",
    "                uuid, major, minor = parsed\n",
    "                results.append((current_mac, uuid, major, minor))\n",
    "            collecting = False\n",
    "\n",
    "        if collecting:\n",
    "            # Printable hex bytes (NN, 0xNN, etc.)\n",
    "            for h in HEX_IN_LINE_RE.finditer(line):\n",
    "                collected.append(int(h.group(1), 16))\n",
    "\n",
    "            # Raw control bytes (0x00–0x1F), preserved via latin-1\n",
    "            raw = line.encode(\"latin1\", errors=\"ignore\")\n",
    "            collected.extend(b for b in raw if b < 32)\n",
    "\n",
    "    # Flush at end of block\n",
    "    if collecting:\n",
    "        parsed = parse_ibeacon_bytes(bytes(collected))\n",
    "        if parsed and current_mac:\n",
    "            uuid, major, minor = parsed\n",
    "            results.append((current_mac, uuid, major, minor))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Apply extraction\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "df[\"mac_beacon_tuples\"] = df[\"revelations_clean\"].apply(extract_ibeacon_tuples)\n",
    "\n",
    "flat_rows = [t for row in df[\"mac_beacon_tuples\"] for t in row]\n",
    "\n",
    "# Keep full tuple uniqueness: (mac, uuid, major, minor)\n",
    "mac_beacon_df = (\n",
    "    pd.DataFrame(flat_rows, columns=[\"mac\", \"uuid\", \"major\", \"minor\"])\n",
    "      .drop_duplicates(subset=[\"mac\", \"uuid\", \"major\", \"minor\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PRINT RESULTS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\n===================================================\")\n",
    "print(\"   iBEACON SUMMARY (MAC / UUID / MAJOR / MINOR)\")\n",
    "print(\"===================================================\\n\")\n",
    "\n",
    "# Ensure DataFrame exists\n",
    "if \"mac_beacon_df\" not in globals():\n",
    "    raise NameError(\"mac_beacon_df is not defined.\")\n",
    "\n",
    "total_ibeacon_devices = len(mac_beacon_df)\n",
    "\n",
    "print(f\"Total iBeacon devices (MAC/UUID/Major/Minor): {total_ibeacon_devices}\\n\")\n",
    "\n",
    "print(\"===================================================\")\n",
    "print(\"        iBEACON DEVICES (MAC / UUID / MAJOR / MINOR)\")\n",
    "print(\"===================================================\\n\")\n",
    "\n",
    "if not mac_beacon_df.empty:\n",
    "    print(mac_beacon_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No iBeacon devices found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad52a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "\"\"\"\n",
    "Builds a beacon-only presence table by filtering observed MAC addresses to those in mac_beacon_df, converting df[\"timestamp_client\"]\n",
    "to UTC timestamps, constructing a deduplicated (mac, ts) observation set from df[\"mac_list\"], segmenting each MAC’s timeline into\n",
    "sessions using a fixed inactivity gap threshold (GAP_SECONDS), and computing per-MAC first_seen, last_seen, number of sessions, \n",
    "total presence time (sum of session durations), and total absence time (sum of inter-observation gaps exceeding the threshold) \n",
    "with scalar outputs in seconds.\n",
    "\"\"\"\n",
    "# =========================\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Config\n",
    "# -------------------------------------------------\n",
    "GAP_SECONDS = 180\n",
    "GAP = pd.Timedelta(seconds=GAP_SECONDS)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Beacon MAC inventory \n",
    "# -------------------------------------------------\n",
    "if \"mac_beacon_df\" not in globals() or mac_beacon_df.empty:\n",
    "    raise ValueError(\"mac_beacon_df is missing or empty — no iBeacon MACs available\")\n",
    "\n",
    "beacon_macs = set(\n",
    "    mac_beacon_df[\"mac\"]\n",
    "    .astype(str)\n",
    "    .str.upper()\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Parse timestamps\n",
    "# -------------------------------------------------\n",
    "df = df.copy()\n",
    "df[\"ts_utc\"] = pd.to_datetime(\n",
    "    df[\"timestamp_client\"], utc=True, errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Ensure mac_list exists\n",
    "# -------------------------------------------------\n",
    "if \"mac_list\" not in df.columns:\n",
    "    def _extract_mac_list(block):\n",
    "        if not isinstance(block, str):\n",
    "            return []\n",
    "        return [m.upper() for m in MAC_RE.findall(block)]\n",
    "\n",
    "    df[\"mac_list\"] = df[\"revelations_clean\"].apply(_extract_mac_list)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Build beacon observations (MAC + timestamp)\n",
    "# -------------------------------------------------\n",
    "obs_rows = []\n",
    "\n",
    "for ts, macs in zip(df[\"ts_utc\"], df[\"mac_list\"]):\n",
    "    if pd.isna(ts) or not isinstance(macs, list):\n",
    "        continue\n",
    "\n",
    "    for mac in macs:\n",
    "        mac_u = mac.upper()\n",
    "        if mac_u in beacon_macs:\n",
    "            obs_rows.append((mac_u, ts))\n",
    "\n",
    "obs = (\n",
    "    pd.DataFrame(obs_rows, columns=[\"mac\", \"ts\"])\n",
    "      .drop_duplicates()\n",
    "      .sort_values([\"mac\", \"ts\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Compute session-based metrics\n",
    "# -------------------------------------------------\n",
    "metrics = []\n",
    "\n",
    "for mac, g in obs.groupby(\"mac\", sort=False):\n",
    "    t = g[\"ts\"].sort_values().reset_index(drop=True)\n",
    "    if t.empty:\n",
    "        continue\n",
    "\n",
    "    first_seen = t.iloc[0]\n",
    "    last_seen  = t.iloc[-1]\n",
    "\n",
    "    gaps = t.diff()\n",
    "\n",
    "    # New session when gap > GAP\n",
    "    is_new_session = gaps.isna() | (gaps > GAP)\n",
    "    session_ids = is_new_session.cumsum()\n",
    "\n",
    "    session_starts = t.groupby(session_ids).first()\n",
    "    session_ends   = t.groupby(session_ids).last()\n",
    "\n",
    "    session_durations = session_ends - session_starts\n",
    "\n",
    "    total_presence = session_durations.sum()\n",
    "    total_absence  = gaps[gaps > GAP].sum()\n",
    "\n",
    "    metrics.append({\n",
    "        \"mac\": mac,\n",
    "        \"first_seen\": first_seen,\n",
    "        \"last_seen\": last_seen,\n",
    "        \"num_sessions\": int(len(session_durations)),\n",
    "        \"total_presence_seconds\": float(total_presence.total_seconds()),\n",
    "        \"total_absence_seconds\": float(total_absence.total_seconds()),\n",
    "    })\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Final table\n",
    "# -------------------------------------------------\n",
    "df_beacon_metrics = (\n",
    "    pd.DataFrame(metrics)\n",
    "      .sort_values([\"first_seen\", \"mac\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Human-friendly index\n",
    "df_beacon_metrics.index = df_beacon_metrics.index + 1\n",
    "\n",
    "df_beacon_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1040bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "Constructs per-MAC session intervals from an observation table containing [\"mac\", \"ts\"] by normalizing timestamps to datetime, \n",
    "removing invalid rows, ordering observations per MAC, and segmenting sequences into sessions using a fixed inactivity\n",
    "threshold. For each session, computes session_start and session_end based on first and last timestamps within contiguous segments,\n",
    "aggregates results into df_beacon_session_detail sorted, and prints the session timeline per MAC.\n",
    "\"\"\"\n",
    "\n",
    "# --- Safety: ensure ts is datetime ---\n",
    "obs = obs.copy()\n",
    "obs[\"ts\"] = pd.to_datetime(obs[\"ts\"], errors=\"coerce\")\n",
    "obs = obs.dropna(subset=[\"mac\", \"ts\"])\n",
    "\n",
    "session_rows = []\n",
    "\n",
    "# --- Build per-session start/end for each MAC ---\n",
    "for mac, g in obs.groupby(\"mac\", sort=False):\n",
    "    t = g[\"ts\"].sort_values().reset_index(drop=True)\n",
    "    if t.empty:\n",
    "        continue\n",
    "\n",
    "    gaps = t.diff()\n",
    "\n",
    "    # New session when first row OR gap > GAP\n",
    "    session_id = (gaps.isna() | (gaps > GAP)).cumsum()  # 1..N per MAC\n",
    "\n",
    "    starts = t.groupby(session_id).first()\n",
    "    ends   = t.groupby(session_id).last()\n",
    "\n",
    "    for sid in starts.index:\n",
    "        session_rows.append({\n",
    "            \"mac\": mac,\n",
    "            \"session_id\": int(sid),\n",
    "            \"session_start\": starts.loc[sid],\n",
    "            \"session_end\": ends.loc[sid],\n",
    "        })\n",
    "\n",
    "df_beacon_session_detail = (\n",
    "    pd.DataFrame(session_rows)\n",
    "      .sort_values([\"mac\", \"session_id\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# --- Print per-MAC session list ---\n",
    "for mac, g in df_beacon_session_detail.groupby(\"mac\", sort=False):\n",
    "    total_sessions = int(g[\"session_id\"].max()) if not g.empty else 0\n",
    "    print(f\"\\nMAC: {mac} | total_sessions: {total_sessions}\")\n",
    "    for _, r in g.iterrows():\n",
    "        print(f\"  Session {int(r['session_id'])}: {r['session_start']} | {r['session_end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "\"\"\"\n",
    "Constructs a session-level observation dataset restricted to iBeacon MAC addresses by normalizing timestamps to UTC, \n",
    "validating and filtering usable rows, standardizing mac_list entries, exploding per-row MAC observations, and retaining \n",
    "only MACs present in beacon_macs. Observations are deduplicated and chronologically ordered per MAC, segmented into sessions\n",
    "using a fixed inactivity threshold, and assigned incremental session_id values. For each observation, only log lines\n",
    "referencing the corresponding MAC are retained. The code produces both a detailed session-observation table and an aggregated \n",
    "session summary, and prints the full per-session timeline including timestamp and MAC-specific payload content.\n",
    "\"\"\"\n",
    "# =========================\n",
    "\n",
    "# --- Ensure GAP is a Timedelta\n",
    "if not isinstance(GAP, pd.Timedelta):\n",
    "    GAP = pd.to_timedelta(GAP)\n",
    "\n",
    "# --- Copy + parse timestamps (UTC) ---\n",
    "df = df.copy()\n",
    "df[\"ts_utc\"] = pd.to_datetime(df[\"timestamp_client\"], utc=True, errors=\"coerce\")\n",
    "df = df[df[\"ts_utc\"].notna() & df[\"revelations_clean\"].notna()].copy()\n",
    "\n",
    "# --- Normalize beacon_macs to uppercase string ---\n",
    "beacon_macs = {str(m).upper() for m in beacon_macs if pd.notna(m)}\n",
    "\n",
    "# --- Normalize mac_list ---\n",
    "def _normalize_mac_list(x):\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    # if it's a string, try splitting on common separators\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        # handles \"['AA', 'BB']\"; keep simple\n",
    "        for sep in [\"|\", \";\", \",\", \" \"]:\n",
    "            if sep in s:\n",
    "                parts = [p for p in (pp.strip() for pp in s.split(sep)) if p]\n",
    "                if len(parts) > 1:\n",
    "                    return parts\n",
    "        return [s]\n",
    "    return [x]\n",
    "\n",
    "df[\"mac_list_norm\"] = df[\"mac_list\"].apply(_normalize_mac_list)\n",
    "\n",
    "# --- Each observation keeps the revelations_clean from that MQTT row ---\n",
    "obs_detail = (\n",
    "    df[[\"ts_utc\", \"revelations_clean\", \"mac_list_norm\"]]\n",
    "      .explode(\"mac_list_norm\")\n",
    "      .rename(columns={\"mac_list_norm\": \"mac\"})\n",
    ")\n",
    "\n",
    "# --- Normalize MAC + filter to iBeacon MACs only ---\n",
    "obs_detail[\"mac\"] = obs_detail[\"mac\"].astype(str).str.upper().str.strip()\n",
    "obs_detail = obs_detail[obs_detail[\"mac\"].isin(beacon_macs)].copy()\n",
    "\n",
    "# --- Drop duplicates + sort ---\n",
    "obs_detail = (\n",
    "    obs_detail.drop_duplicates(subset=[\"mac\", \"ts_utc\"])\n",
    "              .sort_values([\"mac\", \"ts_utc\"])\n",
    "              .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# --- Assign session_id per MAC after GAP ---\n",
    "def _assign_sessions(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = group.sort_values(\"ts_utc\").copy()\n",
    "    gaps = g[\"ts_utc\"].diff()\n",
    "    g[\"session_id\"] = (gaps.isna() | (gaps > GAP)).cumsum().astype(int)  # 1..N\n",
    "    return g\n",
    "\n",
    "df_beacon_session_obs = (\n",
    "    obs_detail.groupby(\"mac\", group_keys=False, sort=False)\n",
    "              .apply(_assign_sessions)\n",
    "              .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# --- Keep only lines that mention this MAC ---\n",
    "def _keep_mac_lines(block, mac: str) -> str:\n",
    "    if not isinstance(block, str):\n",
    "        return \"\"\n",
    "    mac_u = str(mac).upper()\n",
    "    out = []\n",
    "    for ln in block.splitlines():\n",
    "        if mac_u in ln.upper():\n",
    "            out.append(ln)\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "df_beacon_session_obs[\"revelations_clean_mac_only\"] = df_beacon_session_obs.apply(\n",
    "    lambda r: _keep_mac_lines(r[\"revelations_clean\"], r[\"mac\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --- Session summary (start/end/count) ---\n",
    "df_beacon_session_summary = (\n",
    "    df_beacon_session_obs.groupby([\"mac\", \"session_id\"], as_index=False)\n",
    "    .agg(\n",
    "        session_start=(\"ts_utc\", \"min\"),\n",
    "        session_end=(\"ts_utc\", \"max\"),\n",
    "        num_observations=(\"ts_utc\", \"count\"),\n",
    "    )\n",
    "    .sort_values([\"mac\", \"session_id\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# --- PRINT: per MAC -> per session -> each timestamp + payload text ---\n",
    "TEXT_FIELD = \"revelations_clean_mac_only\"\n",
    "\n",
    "for mac, gmac in df_beacon_session_obs.groupby(\"mac\", sort=False):\n",
    "    total_sessions = int(gmac[\"session_id\"].max()) if not gmac.empty else 0\n",
    "    print(f\"\\nMAC: {mac} | total_sessions: {total_sessions}\")\n",
    "\n",
    "    for sid, gs in gmac.groupby(\"session_id\", sort=True):\n",
    "        s_start = gs[\"ts_utc\"].min()\n",
    "        s_end   = gs[\"ts_utc\"].max()\n",
    "        print(f\"  Session {int(sid)}: {s_start} -> {s_end} | observations: {len(gs)}\")\n",
    "\n",
    "        for _, r in gs.iterrows():\n",
    "            print(f\"    - {r['ts_utc']}\")\n",
    "            txt = r[TEXT_FIELD]\n",
    "            print(txt if isinstance(txt, str) else \"\")\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef5a7d-a1e4-40ae-a154-647bba6b333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implements a Tkinter GUI to inspect iBeacon tuples, session presence, and raw log analytics by MAC address; validates and normalizes \n",
    "input DataFrames, applies a global MAC filter across tabs, displays summary tables and session event text, computes per-MAC RSSI \n",
    "samples and binned NEW/CHG/DEL counts with caching, classifies proximity and movement from RSSI using thresholds and \n",
    "MAD-based rolling dispersion, embeds Matplotlib plots with consistent styling, and exports the selected plot image and its source data.\n",
    "\"\"\"\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from tkinter import filedialog, messagebox\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "\n",
    "# ---- Universal plot defaults (applies to all figures) ----\n",
    "IEEE_WIDTH = 3.5   # inches (single column)\n",
    "IEEE_HEIGHT = 2.5  # inches \n",
    "\n",
    "matplotlib.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times New Roman\", \"Times\", \"STIXGeneral\", \"TeX Gyre Termes\"],\n",
    "    \"mathtext.fontset\": \"stix\",\n",
    "    \"axes.unicode_minus\": False,\n",
    "    \"pdf.use14corefonts\": True,\n",
    "\n",
    "    \"axes.labelsize\": 9,\n",
    "    \"xtick.labelsize\": 8,\n",
    "    \"ytick.labelsize\": 8,\n",
    "    \"legend.fontsize\": 6,\n",
    "    \"axes.titlesize\": 9,\n",
    "\n",
    "    \"axes.linewidth\": 0.9,\n",
    "    \"lines.linewidth\": 1.4,\n",
    "    \"grid.linewidth\": 0.5,\n",
    "    \"xtick.major.width\": 0.6,\n",
    "    \"ytick.major.width\": 0.6,\n",
    "})\n",
    "\n",
    "# Proximity thresholds (RSSI in dBm)\n",
    "NEAR_RSSI_THRESHOLD = -50\n",
    "FAR_RSSI_THRESHOLD = -75\n",
    "\n",
    "# Movement thresholds\n",
    "STATIC_STD_MAX = 3.0\n",
    "MOVING_STD_MIN = 4.0\n",
    "MOVING_STEP_MIN = 3.5\n",
    "\n",
    "# Analytics defaults\n",
    "EVENT_BIN_FREQ = \"5min\" \n",
    "ROLLING_WINDOW = 30\n",
    "\n",
    "EVENT_TAG_RE = re.compile(r\"^\\s*\\[(NEW|CHG|DEL)\\]\", re.MULTILINE)\n",
    "RSSI_RE = re.compile(r\"RSSI\\s*:\\s*(-?\\d+)\", re.IGNORECASE)\n",
    "\n",
    "from zoneinfo import ZoneInfo\n",
    "UTC_TZ = ZoneInfo(\"UTC\")\n",
    "\n",
    "\n",
    "def _binfreq_minutes(freq: str) -> int:\n",
    "    \"\"\"\n",
    "    Convert pandas 'T' minute-based frequency strings to integer minutes for labels.\n",
    "    \"\"\"\n",
    "    if not isinstance(freq, str):\n",
    "        return None\n",
    "    m = re.fullmatch(r\"\\s*(\\d+)\\s*T\\s*\", freq, flags=re.IGNORECASE)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "\n",
    "class BeaconDeepViewer(tk.Tk):\n",
    "    _RSSI_RE = re.compile(r\"RSSI\\s*:\\s*(-?\\d+)\", re.IGNORECASE)\n",
    "\n",
    "    def __init__(self, df_tuples: pd.DataFrame, df_sessions: pd.DataFrame, df_raw: pd.DataFrame):\n",
    "        super().__init__()\n",
    "        self.title(\"iBeacon Session Inspector\")\n",
    "        self.geometry(\"1300x860\")\n",
    "        self.minsize(1100, 720)\n",
    "        self.resizable(True, True)\n",
    "\n",
    "        self.df_tuples = df_tuples.copy()\n",
    "        self.df_sessions = df_sessions.copy()\n",
    "        self.df_raw = df_raw.copy()\n",
    "\n",
    "        self.uuid_col = None\n",
    "        self._current_mac = None\n",
    "\n",
    "        # Global filter state\n",
    "        self._filter_text = \"\"\n",
    "        self._filter_upper = \"\"\n",
    "        self._all_macs = []\n",
    "        self._ui_ready = False\n",
    "\n",
    "        # Analytics state\n",
    "        self._analytics_cache = {} \n",
    "        \n",
    "        self._analytics_tmin = None\n",
    "        self._analytics_tmax = None\n",
    "\n",
    "        self.TIME_AXIS_LABEL = \"Observation Timestamp (UTC)\"\n",
    "        self.TIME_TICK_FMT = \"%H:%M\"\n",
    "        self.EVENT_BIN_MIN = 5\n",
    "        self.EVENT_BIN_LABEL = f\"{self.EVENT_BIN_MIN} min\"\n",
    "        self._plot_data = {} \n",
    "\n",
    "\n",
    "\n",
    "        # Legend style \n",
    "        self.LEGEND_KW = {\n",
    "            \"loc\": \"upper right\",\n",
    "            \"frameon\": True,\n",
    "            \"fontsize\": 6,\n",
    "            \"borderaxespad\": 0.4,\n",
    "            \"handlelength\": 1.5,\n",
    "            \"handletextpad\": 0.4,\n",
    "        }\n",
    "        \n",
    "        self.PLOT_STYLE = {\n",
    "            \"title_size\": 9,\n",
    "            \"label_size\": 9,\n",
    "            \"tick_size\": 8,\n",
    "            \"legend_size\": self.LEGEND_KW[\"fontsize\"],\n",
    "        }\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # Universal plot style controls\n",
    "        # ---------------------------------------------------------\n",
    "\n",
    "        self._current_plot_name = None\n",
    "        self._current_plot_mac = None\n",
    "        self._current_plot_df = None \n",
    "\n",
    "        self._prepare_inputs()\n",
    "        self._build_ui()\n",
    "        self._ui_ready = True\n",
    "        self._apply_filter(\"\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Data preparation\n",
    "    # ---------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _normalize_mac_list(x):\n",
    "        if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "            return []\n",
    "        if isinstance(x, (list, tuple, set)):\n",
    "            return [str(m).upper().strip() for m in x if pd.notna(m) and str(m).strip()]\n",
    "        if isinstance(x, str):\n",
    "            s = x.strip()\n",
    "            if not s:\n",
    "                return []\n",
    "            for sep in [\"|\", \";\", \",\", \" \"]:\n",
    "                if sep in s:\n",
    "                    parts = [p.strip() for p in s.split(sep) if p.strip()]\n",
    "                    if len(parts) > 1:\n",
    "                        return [p.upper() for p in parts]\n",
    "            return [s.upper()]\n",
    "        return [str(x).upper().strip()]\n",
    "\n",
    "    def _prepare_inputs(self):\n",
    "        # ---- tuples\n",
    "        if \"mac\" not in self.df_tuples.columns:\n",
    "            raise KeyError(\"df_tuples must contain 'mac'\")\n",
    "    \n",
    "        if \"beacon_uuid\" in self.df_tuples.columns:\n",
    "            self.uuid_col = \"beacon_uuid\"\n",
    "        elif \"uuid\" in self.df_tuples.columns:\n",
    "            self.uuid_col = \"uuid\"\n",
    "        else:\n",
    "            raise KeyError(\"df_tuples must contain 'beacon_uuid' or 'uuid'\")\n",
    "    \n",
    "        for c in (\"major\", \"minor\"):\n",
    "            if c not in self.df_tuples.columns:\n",
    "                raise KeyError(f\"df_tuples must contain '{c}'\")\n",
    "    \n",
    "        self.df_tuples[\"mac\"] = self.df_tuples[\"mac\"].astype(str).str.upper().str.strip()\n",
    "        self.df_tuples[self.uuid_col] = self.df_tuples[self.uuid_col].astype(str).str.strip().str.lower()\n",
    "        self.df_tuples[\"major\"] = pd.to_numeric(self.df_tuples[\"major\"], errors=\"coerce\")\n",
    "        self.df_tuples[\"minor\"] = pd.to_numeric(self.df_tuples[\"minor\"], errors=\"coerce\")\n",
    "    \n",
    "        self.df_tuples = self.df_tuples.dropna(subset=[\"mac\", self.uuid_col, \"major\", \"minor\"]).copy()\n",
    "        self.df_tuples[\"major\"] = self.df_tuples[\"major\"].astype(int)\n",
    "        self.df_tuples[\"minor\"] = self.df_tuples[\"minor\"].astype(int)\n",
    "    \n",
    "        self.df_tuples = (\n",
    "            self.df_tuples\n",
    "            .drop_duplicates(subset=[\"mac\", self.uuid_col, \"major\", \"minor\"])\n",
    "            .sort_values([\"mac\", self.uuid_col, \"major\", \"minor\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    \n",
    "        self._list_macs = sorted(self.df_tuples[\"mac\"].unique().tolist())\n",
    "        self._list_uuids = sorted(self.df_tuples[self.uuid_col].unique().tolist())\n",
    "        self._list_majors = sorted(self.df_tuples[\"major\"].unique().tolist())\n",
    "        self._list_minors = sorted(self.df_tuples[\"minor\"].unique().tolist())\n",
    "        self._all_macs = list(self._list_macs)\n",
    "    \n",
    "        # ---- sessions \n",
    "        required = {\"mac\", \"session_id\", \"session_start\", \"session_end\"}\n",
    "        if not required.issubset(self.df_sessions.columns):\n",
    "            raise KeyError(f\"df_sessions must contain {required}\")\n",
    "    \n",
    "        self.df_sessions[\"mac\"] = self.df_sessions[\"mac\"].astype(str).str.upper().str.strip()\n",
    "    \n",
    "        s0 = pd.to_datetime(self.df_sessions[\"session_start\"], errors=\"coerce\")\n",
    "        s1 = pd.to_datetime(self.df_sessions[\"session_end\"], errors=\"coerce\")\n",
    "    \n",
    "        # If tz-naive, assume already UTC\n",
    "        self.df_sessions[\"session_start\"] = (\n",
    "            s0.dt.tz_localize(\"UTC\", nonexistent=\"shift_forward\", ambiguous=\"NaT\")\n",
    "            if s0.dt.tz is None else s0.dt.tz_convert(\"UTC\")\n",
    "        )\n",
    "        self.df_sessions[\"session_end\"] = (\n",
    "            s1.dt.tz_localize(\"UTC\", nonexistent=\"shift_forward\", ambiguous=\"NaT\")\n",
    "            if s1.dt.tz is None else s1.dt.tz_convert(\"UTC\")\n",
    "        )\n",
    "    \n",
    "        self.df_sessions = self.df_sessions.dropna(subset=[\"session_start\", \"session_end\"]).copy()\n",
    "    \n",
    "        if \"revelations_clean\" not in self.df_raw.columns:\n",
    "            raise KeyError(\"df_raw must contain 'revelations_clean'\")\n",
    "        if \"mac_list\" not in self.df_raw.columns:\n",
    "            raise KeyError(\"df_raw must contain 'mac_list'\")\n",
    "    \n",
    "        ROME_TZ = ZoneInfo(\"Europe/Rome\")\n",
    "    \n",
    "        if \"ts_utc\" in self.df_raw.columns:\n",
    "            ts = pd.to_datetime(self.df_raw[\"ts_utc\"], errors=\"coerce\")\n",
    "            self.df_raw[\"ts_utc\"] = (\n",
    "                ts.dt.tz_localize(\"UTC\", nonexistent=\"shift_forward\", ambiguous=\"NaT\")\n",
    "                if ts.dt.tz is None else ts.dt.tz_convert(\"UTC\")\n",
    "            )\n",
    "        else:\n",
    "            if \"timestamp_client\" in self.df_raw.columns:\n",
    "                ts = pd.to_datetime(self.df_raw[\"timestamp_client\"], errors=\"coerce\")\n",
    "                self.df_raw[\"ts_utc\"] = (\n",
    "                    ts.dt.tz_localize(ROME_TZ, nonexistent=\"shift_forward\", ambiguous=\"NaT\").dt.tz_convert(\"UTC\")\n",
    "                    if ts.dt.tz is None else ts.dt.tz_convert(\"UTC\")\n",
    "                )\n",
    "            else:\n",
    "                raise KeyError(\"df_raw must contain 'ts_utc' or 'timestamp_client'\")\n",
    "    \n",
    "        self.df_raw = self.df_raw.dropna(subset=[\"ts_utc\"]).copy()\n",
    "        self.df_raw[\"mac_list\"] = self.df_raw[\"mac_list\"].apply(self._normalize_mac_list)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Global filter\n",
    "    # ---------------------------------------------------------\n",
    "    def _apply_filter(self, text: str):\n",
    "        text = (text or \"\").strip()\n",
    "        self._filter_text = text\n",
    "        self._filter_upper = text.upper()\n",
    "        if self._ui_ready:\n",
    "            self._refresh_all_tabs()\n",
    "\n",
    "    def _filtered_macs(self):\n",
    "        if not self._filter_upper:\n",
    "            return list(self._all_macs)\n",
    "        return [m for m in self._all_macs if self._filter_upper in m]\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # iBeacon Summary text \n",
    "    # ---------------------------------------------------------\n",
    "    def _iBeacon_text_filtered(self, df_subset: pd.DataFrame) -> str:\n",
    "        lines = []\n",
    "        lines.append(\"iBeacon Summary\\n\")\n",
    "\n",
    "        if df_subset.empty:\n",
    "            lines.append(\"No records for current filter.\")\n",
    "            return \"\\n\".join(lines)\n",
    "\n",
    "        list_macs = sorted(df_subset[\"mac\"].unique().tolist())\n",
    "        list_uuids = sorted(df_subset[self.uuid_col].unique().tolist())\n",
    "        list_majors = sorted(df_subset[\"major\"].unique().tolist())\n",
    "        list_minors = sorted(df_subset[\"minor\"].unique().tolist())\n",
    "\n",
    "        lines.append(f\"Active MAC filter: {self._filter_text!r}\" if self._filter_text else \"Active MAC filter: (none)\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(f\"Total unique iBeacon identities (MAC|UUID|Major|Minor): {len(df_subset)}\")\n",
    "        lines.append(f\"Unique MACs          : {len(list_macs)}\")\n",
    "        lines.append(f\"Unique UUIDs         : {len(list_uuids)}\")\n",
    "        lines.append(f\"Unique Majors        : {len(list_majors)}\")\n",
    "        lines.append(f\"Unique Minors        : {len(list_minors)}\")\n",
    "        lines.append(\"\")\n",
    "\n",
    "        lines.append(\"---- MAC Addresses ----\")\n",
    "        lines.extend(f\"  {m}\" for m in list_macs)\n",
    "        lines.append(\"\")\n",
    "\n",
    "        lines.append(\"---- UUIDs ----\")\n",
    "        lines.extend(f\"  {u}\" for u in list_uuids)\n",
    "        lines.append(\"\")\n",
    "\n",
    "        lines.append(\"---- Majors ----\")\n",
    "        lines.extend(f\"  {m}\" for m in list_majors)\n",
    "        lines.append(\"\")\n",
    "\n",
    "        lines.append(\"---- Minors ----\")\n",
    "        lines.extend(f\"  {m}\" for m in list_minors)\n",
    "\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Parsing \n",
    "    # ---------------------------------------------------------\n",
    "    @classmethod\n",
    "    def _parse_rssi_from_text(cls, text: str):\n",
    "        if not isinstance(text, str):\n",
    "            return []\n",
    "        vals = []\n",
    "        for m in cls._RSSI_RE.finditer(text):\n",
    "            try:\n",
    "                vals.append(int(m.group(1)))\n",
    "            except Exception:\n",
    "                pass\n",
    "        return vals\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Movement detection (MAD-based)\n",
    "    # ---------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _detect_moving_or_static_mad(rssi_values):\n",
    "        rssi_values = [int(v) for v in (rssi_values or []) if v is not None]\n",
    "        n = len(rssi_values)\n",
    "        if n < 5:\n",
    "            return (\"STATIC\", None, None, n)\n",
    "\n",
    "        r = np.array(rssi_values, dtype=float)\n",
    "\n",
    "        med = float(np.median(r))\n",
    "        mad = float(np.median(np.abs(r - med)))\n",
    "        std = 1.4826 * mad\n",
    "\n",
    "        # Keep step only as a *reported* metric\n",
    "        median_step = float(np.median(np.abs(np.diff(r)))) if n >= 2 else 0.0\n",
    "\n",
    "        # Decision based primarily on MAD-std (robust spread)\n",
    "        if std <= STATIC_STD_MAX:\n",
    "            return (\"STATIC\", std, median_step, n)\n",
    "\n",
    "        if std >= MOVING_STD_MIN:\n",
    "            return (\"MOVING\", std, median_step, n)\n",
    "\n",
    "        # MID-zone: step is only a tie-breaker\n",
    "        if median_step >= MOVING_STEP_MIN:\n",
    "            return (\"MOVING\", std, median_step, n)\n",
    "\n",
    "        return (\"STATIC\", std, median_step, n)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Proximity label\n",
    "    # ---------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _proximity_label(rssi: int) -> str:\n",
    "        try:\n",
    "            r = int(rssi)\n",
    "        except Exception:\n",
    "            return None\n",
    "        if r >= NEAR_RSSI_THRESHOLD:\n",
    "            return \"NEAR\"\n",
    "        if r <= FAR_RSSI_THRESHOLD:\n",
    "            return \"FAR\"\n",
    "        return \"MID\"\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # MAC-scoped event extraction \n",
    "    # ---------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def extract_events_for_mac(block: str, mac: str) -> str:\n",
    "        if not isinstance(block, str):\n",
    "            return \"\"\n",
    "        mac = mac.upper()\n",
    "        out = []\n",
    "        capturing = False\n",
    "        for ln in block.splitlines():\n",
    "            if ln.lstrip().startswith(\"[\"):\n",
    "                if mac in ln.upper():\n",
    "                    capturing = True\n",
    "                    out.append(ln)\n",
    "                else:\n",
    "                    capturing = False\n",
    "            else:\n",
    "                if capturing:\n",
    "                    out.append(ln)\n",
    "        return \"\\n\".join(out).strip()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Presence summary computation \n",
    "    # ---------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _fmt_tdelta_hms(td) -> str:\n",
    "        if td is None or pd.isna(td):\n",
    "            return \"\"\n",
    "        try:\n",
    "            total_seconds = int(td.total_seconds())\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "        sign = \"-\" if total_seconds < 0 else \"\"\n",
    "        total_seconds = abs(total_seconds)\n",
    "        h = total_seconds // 3600\n",
    "        m = (total_seconds % 3600) // 60\n",
    "        s = total_seconds % 60\n",
    "        return f\"{sign}{h}:{m:02d}:{s:02d}\"\n",
    "\n",
    "    def _compute_presence_summary_filtered(self, macs):\n",
    "        if self.df_sessions.empty or not macs:\n",
    "            return pd.DataFrame(columns=[\n",
    "                \"mac\", \"first_seen\", \"last_seen\",\n",
    "                \"number_of_sessions\", \"total_present_time\", \"total_absent_time\"\n",
    "            ])\n",
    "\n",
    "        ds = self.df_sessions[self.df_sessions[\"mac\"].isin(macs)].copy()\n",
    "\n",
    "        out_rows = []\n",
    "        for mac, g in ds.groupby(\"mac\", sort=False):\n",
    "            g = g.sort_values(\"session_start\").reset_index(drop=True)\n",
    "            first_seen = g[\"session_start\"].min()\n",
    "            last_seen = g[\"session_end\"].max()\n",
    "            num_sessions = int(g.shape[0])\n",
    "            present = (g[\"session_end\"] - g[\"session_start\"]).sum()\n",
    "\n",
    "            gaps = (g[\"session_start\"].shift(-1) - g[\"session_end\"]).dropna()\n",
    "            gaps = gaps[gaps > pd.Timedelta(0)]\n",
    "            absent = gaps.sum() if not gaps.empty else pd.Timedelta(0)\n",
    "\n",
    "            out_rows.append({\n",
    "                \"mac\": mac,\n",
    "                \"first_seen\": first_seen,\n",
    "                \"last_seen\": last_seen,\n",
    "                \"number_of_sessions\": num_sessions,\n",
    "                \"total_present_time\": present,\n",
    "                \"total_absent_time\": absent\n",
    "            })\n",
    "\n",
    "        dfp = pd.DataFrame(out_rows)\n",
    "        return dfp.sort_values(\"mac\").reset_index(drop=True) if not dfp.empty else dfp\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Analytics dataset (MAC-scoped RSSI + binned event counts)\n",
    "    # ---------------------------------------------------------\n",
    "    def _build_analytics_dataset(self, mac: str):\n",
    "        mac = (mac or \"\").upper().strip()\n",
    "        if not mac:\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "        if mac in self._analytics_cache:\n",
    "            return self._analytics_cache[mac]\n",
    "\n",
    "        mask = self.df_raw[\"mac_list\"].apply(lambda xs: mac in (xs or []))\n",
    "        rows = self.df_raw.loc[mask].sort_values(\"ts_utc\")\n",
    "\n",
    "        samples = []\n",
    "        event_rows = []\n",
    "\n",
    "        for _, rr in rows.iterrows():\n",
    "            ts = rr[\"ts_utc\"]\n",
    "            block = rr[\"revelations_clean\"]\n",
    "            mac_scoped = self.extract_events_for_mac(block, mac)\n",
    "            text_to_parse = mac_scoped if mac_scoped else block\n",
    "\n",
    "            new_cnt = chg_cnt = del_cnt = 0\n",
    "            if mac_scoped:\n",
    "                for t in EVENT_TAG_RE.findall(mac_scoped):\n",
    "                    if t == \"NEW\":\n",
    "                        new_cnt += 1\n",
    "                    elif t == \"CHG\":\n",
    "                        chg_cnt += 1\n",
    "                    elif t == \"DEL\":\n",
    "                        del_cnt += 1\n",
    "\n",
    "            rssi_vals = []\n",
    "            if isinstance(text_to_parse, str):\n",
    "                for m in RSSI_RE.finditer(text_to_parse):\n",
    "                    try:\n",
    "                        rssi_vals.append(int(m.group(1)))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "            for v in rssi_vals:\n",
    "                prox = self._proximity_label(v)\n",
    "                if prox is None:\n",
    "                    continue\n",
    "                samples.append({\"ts_utc\": ts, \"rssi\": int(v), \"prox\": prox})\n",
    "\n",
    "            event_rows.append({\n",
    "                \"ts_utc\": ts,\n",
    "                \"NEW\": int(new_cnt),\n",
    "                \"CHG\": int(chg_cnt),\n",
    "                \"DEL\": int(del_cnt),\n",
    "                \"rssi_samples\": int(len(rssi_vals)),\n",
    "            })\n",
    "\n",
    "        df_s = pd.DataFrame(samples)\n",
    "        df_e = pd.DataFrame(event_rows)\n",
    "\n",
    "        if not df_e.empty:\n",
    "            binned = (\n",
    "                df_e.sort_values(\"ts_utc\")\n",
    "                   .set_index(\"ts_utc\")\n",
    "                   .resample(EVENT_BIN_FREQ)\n",
    "                   .sum(numeric_only=True)\n",
    "                   .reset_index()\n",
    "            )\n",
    "        else:\n",
    "            binned = pd.DataFrame(columns=[\"ts_utc\", \"NEW\", \"CHG\", \"DEL\", \"rssi_samples\"])\n",
    "\n",
    "        self._analytics_cache[mac] = (df_s, binned)\n",
    "        return df_s, binned\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Movement Timeline plot\n",
    "    # ---------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _mad_std(arr: np.ndarray) -> float:\n",
    "        arr = arr.astype(float)\n",
    "        med = np.median(arr)\n",
    "        mad = np.median(np.abs(arr - med))\n",
    "        return 1.4826 * mad\n",
    "\n",
    "    def _rolling_movement(self, rssi_series: pd.Series, window: int = ROLLING_WINDOW) -> pd.DataFrame:\n",
    "        r = rssi_series.dropna().astype(float)\n",
    "        if r.shape[0] < max(5, window):\n",
    "            return pd.DataFrame(columns=[\"roll_mad_std\", \"label\"])\n",
    "\n",
    "        roll_mad_std = r.rolling(window).apply(lambda x: self._mad_std(np.array(x)), raw=False)\n",
    "\n",
    "        def _med_step(x):\n",
    "            x = np.array(x, dtype=float)\n",
    "            if x.size < 2:\n",
    "                return 0.0\n",
    "            return float(np.median(np.abs(np.diff(x))))\n",
    "\n",
    "        roll_step = r.rolling(window).apply(_med_step, raw=False)\n",
    "\n",
    "        labels = []\n",
    "        for std, step in zip(roll_mad_std.values, roll_step.values):\n",
    "            if np.isnan(std) or np.isnan(step):\n",
    "                labels.append(None)\n",
    "                continue\n",
    "\n",
    "            if std <= STATIC_STD_MAX:\n",
    "                labels.append(\"STATIC\")\n",
    "            elif std >= MOVING_STD_MIN:\n",
    "                labels.append(\"MOVING\")\n",
    "            elif step >= MOVING_STEP_MIN:\n",
    "                labels.append(\"MOVING\")\n",
    "            else:\n",
    "                labels.append(\"STATIC\")\n",
    "\n",
    "        return pd.DataFrame({\"roll_mad_std\": roll_mad_std, \"label\": labels}, index=r.index)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Universal axis styling + exporting\n",
    "    # ---------------------------------------------------------\n",
    "    def _apply_universal_axis_style(self, ax):\n",
    "        ax.title.set_fontsize(self.PLOT_STYLE[\"title_size\"])\n",
    "        ax.xaxis.label.set_fontsize(self.PLOT_STYLE[\"label_size\"])\n",
    "        ax.yaxis.label.set_fontsize(self.PLOT_STYLE[\"label_size\"])\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=self.PLOT_STYLE[\"tick_size\"])\n",
    "        ax.tick_params(axis=\"both\", which=\"minor\", labelsize=self.PLOT_STYLE[\"tick_size\"])\n",
    "    # ---------------------------------------------------------------------------\n",
    "\n",
    "    def _export_current_plot_image(self):\n",
    "        if not self._current_plot_name or self._current_plot_name not in self._plot_tabs:\n",
    "            messagebox.showwarning(\"Export plot\", \"No plot is currently available to export.\")\n",
    "            return\n",
    "\n",
    "        p = self._plot_tabs[self._current_plot_name]\n",
    "        fig = p[\"fig\"]\n",
    "\n",
    "        default_base = f\"{self._current_plot_mac or 'MAC'}_{self._current_plot_name}\".replace(\" \", \"_\")\n",
    "        file_path = filedialog.asksaveasfilename(\n",
    "            title=\"Save plot image\",\n",
    "            defaultextension=\".pdf\",\n",
    "            initialfile=default_base,\n",
    "            filetypes=[\n",
    "                (\"PDF document\", \"*.pdf\"),\n",
    "                (\"PNG image\", \"*.png\"),\n",
    "                (\"JPEG image\", \"*.jpg\"),\n",
    "            ],\n",
    "        )\n",
    "        if not file_path:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            fig.savefig(file_path, dpi=300)\n",
    "            messagebox.showinfo(\"Export plot\", f\"Saved:\\n{file_path}\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Export plot\", f\"Failed to export plot:\\n{e}\")\n",
    "\n",
    "    def _export_current_plot_data(self):\n",
    "        if self._current_plot_df is None or not isinstance(self._current_plot_df, pd.DataFrame):\n",
    "            messagebox.showwarning(\"Export data\", \"No plot data is currently available to export.\")\n",
    "            return\n",
    "\n",
    "        default_base = f\"{self._current_plot_mac or 'MAC'}_{self._current_plot_name}_data\".replace(\" \", \"_\")\n",
    "        file_path = filedialog.asksaveasfilename(\n",
    "            title=\"Save plot data\",\n",
    "            defaultextension=\".csv\",\n",
    "            initialfile=default_base,\n",
    "            filetypes=[\n",
    "                (\"CSV\", \"*.csv\"),\n",
    "                (\"Excel\", \"*.xlsx\"),\n",
    "            ],\n",
    "        )\n",
    "        if not file_path:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            suffix = Path(file_path).suffix.lower()\n",
    "\n",
    "            if suffix == \".xlsx\":\n",
    "                df_out = self._current_plot_df.copy()\n",
    "\n",
    "                # Excel cannot handle timezone-aware datetimes\n",
    "                for col in df_out.select_dtypes(include=[\"datetimetz\"]).columns:\n",
    "                    df_out[col] = df_out[col].dt.tz_localize(None)\n",
    "\n",
    "                df_out.to_excel(file_path, index=False)\n",
    "            else:\n",
    "                self._current_plot_df.to_csv(file_path, index=False)\n",
    "\n",
    "            messagebox.showinfo(\"Export data\", f\"Saved:\\n{file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Export data\", f\"Failed to export data:\\n{e}\")\n",
    "            \n",
    "\n",
    "    def _set_current_plot_context(self, plot_name: str, mac: str, df: pd.DataFrame):\n",
    "        self._current_plot_mac = mac\n",
    "        self._plot_data[plot_name] = df.copy() if isinstance(df, pd.DataFrame) else None\n",
    "        \n",
    "        if self._current_plot_name == plot_name:\n",
    "            self._current_plot_df = self._plot_data[plot_name]\n",
    "    \n",
    "    def _compute_common_time_range(self, df_s: pd.DataFrame, df_bins: pd.DataFrame):\n",
    "        tmin = tmax = None\n",
    "    \n",
    "        if isinstance(df_s, pd.DataFrame) and not df_s.empty and \"ts_utc\" in df_s.columns:\n",
    "            s = pd.to_datetime(df_s[\"ts_utc\"], utc=True, errors=\"coerce\").dropna()\n",
    "            if not s.empty:\n",
    "                tmin, tmax = s.min(), s.max()\n",
    "    \n",
    "        if isinstance(df_bins, pd.DataFrame) and not df_bins.empty and \"ts_utc\" in df_bins.columns:\n",
    "            b = pd.to_datetime(df_bins[\"ts_utc\"], utc=True, errors=\"coerce\").dropna()\n",
    "            if not b.empty:\n",
    "                if tmin is None or b.min() < tmin:\n",
    "                    tmin = b.min()\n",
    "                if tmax is None or b.max() > tmax:\n",
    "                    tmax = b.max()\n",
    "    \n",
    "        self._analytics_tmin, self._analytics_tmax = tmin, tmax\n",
    "    \n",
    "    \n",
    "    def _apply_common_time_xlim(self, ax):\n",
    "        if self._analytics_tmin is None or self._analytics_tmax is None:\n",
    "            return\n",
    "        if self._analytics_tmax >= self._analytics_tmin:\n",
    "            ax.set_xlim(self._analytics_tmin, self._analytics_tmax)\n",
    "    # =========================================================\n",
    "    \n",
    "    def _on_plot_tab_changed(self, event=None):\n",
    "        try:\n",
    "            tab_id = self.plot_nb.select()\n",
    "            tab_text = self.plot_nb.tab(tab_id, \"text\")\n",
    "            self._current_plot_name = tab_text\n",
    "            self._current_plot_df = self._plot_data.get(tab_text)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # UI: Tabs (Global filter)\n",
    "    # ---------------------------------------------------------\n",
    "    def _build_ui(self):\n",
    "        filter_bar = ttk.Frame(self)\n",
    "        filter_bar.pack(fill=\"x\", padx=10, pady=(10, 6))\n",
    "\n",
    "        ttk.Label(filter_bar, text=\"Global MAC filter:\").pack(side=\"left\")\n",
    "\n",
    "        self.filter_var = tk.StringVar()\n",
    "        self.filter_entry = ttk.Entry(filter_bar, textvariable=self.filter_var, width=40)\n",
    "        self.filter_entry.pack(side=\"left\", padx=8)\n",
    "        self.filter_entry.bind(\"<Return>\", lambda e: self._apply_filter(self.filter_var.get()))\n",
    "\n",
    "        ttk.Button(filter_bar, text=\"Apply\", command=lambda: self._apply_filter(self.filter_var.get())).pack(side=\"left\")\n",
    "        ttk.Button(filter_bar, text=\"Clear\", command=self._on_clear_filter).pack(side=\"left\", padx=6)\n",
    "\n",
    "        self.filter_status = ttk.Label(filter_bar, text=\"(no filter)\")\n",
    "        self.filter_status.pack(side=\"left\", padx=12)\n",
    "\n",
    "        nb = ttk.Notebook(self)\n",
    "        nb.pack(fill=\"both\", expand=True)\n",
    "\n",
    "        self.tab_iBeacon = ttk.Frame(nb)\n",
    "        self.tab_presence = ttk.Frame(nb)\n",
    "        self.tab_inspect = ttk.Frame(nb)\n",
    "        self.tab_track = ttk.Frame(nb)\n",
    "        self.tab_analytics = ttk.Frame(nb)\n",
    "\n",
    "        nb.add(self.tab_iBeacon, text=\"iBeacon Summary\")\n",
    "        nb.add(self.tab_presence, text=\"Presence Summary\")\n",
    "        nb.add(self.tab_inspect, text=\"Session Inspector\")\n",
    "        nb.add(self.tab_track, text=\"Track Devices\")\n",
    "        nb.add(self.tab_analytics, text=\"Plots\")\n",
    "\n",
    "        self._build_tab_iBeacon(self.tab_iBeacon)\n",
    "        self._build_tab_presence_summary(self.tab_presence)\n",
    "        self._build_tab_session_inspector(self.tab_inspect)\n",
    "        self._build_tab_track_devices(self.tab_track)\n",
    "        self._build_tab_analytics(self.tab_analytics)\n",
    "\n",
    "    def _on_clear_filter(self):\n",
    "        self.filter_var.set(\"\")\n",
    "        self._apply_filter(\"\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Tab 1: iBeacon Summary\n",
    "    # ---------------------------------------------------------\n",
    "    def _build_tab_iBeacon(self, parent):\n",
    "        paned = ttk.Panedwindow(parent, orient=\"vertical\")\n",
    "        paned.pack(fill=\"both\", expand=True, padx=10, pady=10)\n",
    "\n",
    "        top = ttk.Frame(paned)\n",
    "        bottom = ttk.Frame(paned)\n",
    "        paned.add(top, weight=2)\n",
    "        paned.add(bottom, weight=3)\n",
    "\n",
    "        txt_box = ttk.LabelFrame(top, text=\"iBeacon Summary\")\n",
    "        txt_box.pack(fill=\"both\", expand=True)\n",
    "\n",
    "        self.iBeacon_text = tk.Text(txt_box, wrap=\"word\")\n",
    "        self.iBeacon_text.pack(fill=\"both\", expand=True)\n",
    "        self.iBeacon_text.configure(state=\"disabled\")\n",
    "\n",
    "        tbl_box = ttk.LabelFrame(bottom, text=\"Tuples (MAC | UUID | Major | Minor)\")\n",
    "        tbl_box.pack(fill=\"both\", expand=True)\n",
    "\n",
    "        self.inv_table = ttk.Treeview(\n",
    "            tbl_box, columns=(\"mac\", \"uuid\", \"major\", \"minor\"),\n",
    "            show=\"headings\", height=16\n",
    "        )\n",
    "        for col in (\"mac\", \"uuid\", \"major\", \"minor\"):\n",
    "            self.inv_table.heading(col, text=col)\n",
    "\n",
    "        self.inv_table.column(\"mac\", anchor=\"w\", width=220, stretch=False)\n",
    "        self.inv_table.column(\"uuid\", anchor=\"w\", width=520, stretch=True)\n",
    "        self.inv_table.column(\"major\", anchor=\"center\", width=90, stretch=False)\n",
    "        self.inv_table.column(\"minor\", anchor=\"center\", width=90, stretch=False)\n",
    "\n",
    "        yscroll = ttk.Scrollbar(tbl_box, orient=\"vertical\", command=self.inv_table.yview)\n",
    "        self.inv_table.configure(yscrollcommand=yscroll.set)\n",
    "\n",
    "        self.inv_table.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "        yscroll.pack(side=\"right\", fill=\"y\")\n",
    "\n",
    "    def _refresh_tab_ibeacon(self, macs):\n",
    "        df_sub = self.df_tuples[self.df_tuples[\"mac\"].isin(macs)].copy()\n",
    "\n",
    "        self.iBeacon_text.configure(state=\"normal\")\n",
    "        self.iBeacon_text.delete(\"1.0\", \"end\")\n",
    "        self.iBeacon_text.insert(\"1.0\", self._iBeacon_text_filtered(df_sub))\n",
    "        self.iBeacon_text.configure(state=\"disabled\")\n",
    "\n",
    "        self.inv_table.delete(*self.inv_table.get_children())\n",
    "        for _, r in df_sub.iterrows():\n",
    "            self.inv_table.insert(\"\", \"end\", values=(r[\"mac\"], r[self.uuid_col], int(r[\"major\"]), int(r[\"minor\"])))\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Tab 2: Presence Summary\n",
    "    # ---------------------------------------------------------\n",
    "    def _build_tab_presence_summary(self, parent):\n",
    "        box = ttk.LabelFrame(parent, text=\"iBeacon Presence Summary (from sessions)\")\n",
    "        box.pack(fill=\"both\", expand=True, padx=10, pady=10)\n",
    "\n",
    "        self.pres_table = ttk.Treeview(\n",
    "            box, columns=(\"mac\", \"first\", \"last\", \"sessions\", \"present\", \"absent\"),\n",
    "            show=\"headings\", height=20\n",
    "        )\n",
    "        self.pres_table.heading(\"mac\", text=\"mac\")\n",
    "        self.pres_table.heading(\"first\", text=\"first_seen\")\n",
    "        self.pres_table.heading(\"last\", text=\"last_seen \")\n",
    "        self.pres_table.heading(\"sessions\", text=\"sessions\")\n",
    "        self.pres_table.heading(\"present\", text=\"total_present_time\")\n",
    "        self.pres_table.heading(\"absent\", text=\"total_absent_time\")\n",
    "\n",
    "        self.pres_table.column(\"mac\", anchor=\"w\", width=220, stretch=False)\n",
    "        self.pres_table.column(\"first\", anchor=\"w\", width=260, stretch=True)\n",
    "        self.pres_table.column(\"last\", anchor=\"w\", width=260, stretch=True)\n",
    "        self.pres_table.column(\"sessions\", anchor=\"center\", width=90, stretch=False)\n",
    "        self.pres_table.column(\"present\", anchor=\"center\", width=160, stretch=False)\n",
    "        self.pres_table.column(\"absent\", anchor=\"center\", width=160, stretch=False)\n",
    "\n",
    "        yscroll = ttk.Scrollbar(box, orient=\"vertical\", command=self.pres_table.yview)\n",
    "        self.pres_table.configure(yscrollcommand=yscroll.set)\n",
    "\n",
    "        self.pres_table.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "        yscroll.pack(side=\"right\", fill=\"y\")\n",
    "\n",
    "    def _refresh_tab_presence(self, macs):\n",
    "        self.pres_table.delete(*self.pres_table.get_children())\n",
    "        dfp = self._compute_presence_summary_filtered(macs)\n",
    "        for _, r in dfp.iterrows():\n",
    "            first_disp = r[\"first_seen\"].isoformat() if pd.notna(r[\"first_seen\"]) else \"\"\n",
    "            last_disp  = r[\"last_seen\"].isoformat()  if pd.notna(r[\"last_seen\"]) else \"\"\n",
    "            self.pres_table.insert(\n",
    "                \"\", \"end\",\n",
    "                values=(\n",
    "                    r[\"mac\"],\n",
    "                    first_disp,\n",
    "                    last_disp,\n",
    "                    int(r[\"number_of_sessions\"]),\n",
    "                    self._fmt_tdelta_hms(r[\"total_present_time\"]),\n",
    "                    self._fmt_tdelta_hms(r[\"total_absent_time\"]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Tab 3: Session Inspector\n",
    "    # ---------------------------------------------------------\n",
    "    def _build_tab_session_inspector(self, parent):\n",
    "        top = ttk.Frame(parent)\n",
    "        top.pack(fill=\"x\", padx=10, pady=6)\n",
    "\n",
    "        ttk.Label(top, text=\"Beacon MAC:\").pack(side=\"left\")\n",
    "        self.mac_var = tk.StringVar()\n",
    "\n",
    "        self.mac_combo = ttk.Combobox(top, values=[], textvariable=self.mac_var, state=\"readonly\", width=30)\n",
    "        self.mac_combo.pack(side=\"left\", padx=6)\n",
    "        self.mac_combo.bind(\"<<ComboboxSelected>>\", self.on_mac_selected)\n",
    "\n",
    "        self.meta_label = ttk.Label(top, text=\"\", font=(\"Courier\", 10))\n",
    "        self.meta_label.pack(side=\"left\", padx=18)\n",
    "\n",
    "        MID= ttk.LabelFrame(parent, text=\"Sessions\")\n",
    "        MID.pack(fill=\"both\", expand=True, padx=10, pady=6)\n",
    "\n",
    "        self.session_table = ttk.Treeview(MID,columns=(\"sid\", \"start\", \"end\", \"duration\", \"next_gap\", \"start_utc\", \"end_utc\"),\n",
    "            show=\"headings\",\n",
    "            height=9\n",
    "        )\n",
    "        col_defs = {\n",
    "            \"sid\": (\"sid\", 140),\n",
    "            \"start\": (\"start \", 300),\n",
    "            \"end\": (\"end \", 300),\n",
    "            \"duration\": (\"duration\", 120),\n",
    "            \"next_gap\": (\"next_gap\", 120),\n",
    "            \"start_utc\": (\"start_utc\", 0),\n",
    "            \"end_utc\": (\"end_utc\", 0),\n",
    "        }\n",
    "        for c, (label, w) in col_defs.items():\n",
    "            self.session_table.heading(c, text=label)\n",
    "            self.session_table.column(c, anchor=\"center\", width=w, stretch=(w > 0))\n",
    "        self.session_table.column(\"start_utc\", width=0, stretch=False)\n",
    "        self.session_table.column(\"end_utc\", width=0, stretch=False)\n",
    "\n",
    "        yscroll = ttk.Scrollbar(MID, orient=\"vertical\", command=self.session_table.yview)\n",
    "        self.session_table.configure(yscrollcommand=yscroll.set)\n",
    "        self.session_table.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "        yscroll.pack(side=\"right\", fill=\"y\")\n",
    "\n",
    "        self.session_table.bind(\"<<TreeviewSelect>>\", self.on_session_selected)\n",
    "\n",
    "        bottom = ttk.LabelFrame(parent, text=\"Session (timestamp → events)\")\n",
    "        bottom.pack(fill=\"both\", expand=True, padx=10, pady=6)\n",
    "\n",
    "        self.evidence = tk.Text(bottom, wrap=\"word\")\n",
    "        self.evidence.pack(fill=\"both\", expand=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def _fmt_tdelta(td) -> str:\n",
    "        if td is None or pd.isna(td):\n",
    "            return \"\"\n",
    "        try:\n",
    "            total_seconds = int(td.total_seconds())\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "        sign = \"-\" if total_seconds < 0 else \"\"\n",
    "        total_seconds = abs(total_seconds)\n",
    "        h = total_seconds // 3600\n",
    "        m = (total_seconds % 3600) // 60\n",
    "        s = total_seconds % 60\n",
    "        return f\"{sign}{h}:{m:02d}:{s:02d}\"\n",
    "\n",
    "    def _refresh_tab_inspector(self, macs):\n",
    "        self.mac_combo.configure(values=macs)\n",
    "        cur = (self.mac_var.get() or \"\").upper().strip()\n",
    "        if cur not in macs:\n",
    "            self.mac_var.set(macs[0] if macs else \"\")\n",
    "            cur = self.mac_var.get()\n",
    "\n",
    "        if cur:\n",
    "            self.on_mac_selected()\n",
    "        else:\n",
    "            self._current_mac = None\n",
    "            self.meta_label.config(text=\"\")\n",
    "            self.session_table.delete(*self.session_table.get_children())\n",
    "            self.evidence.delete(\"1.0\", \"end\")\n",
    "\n",
    "    def on_mac_selected(self, event=None):\n",
    "        mac = (self.mac_var.get() or \"\").upper().strip()\n",
    "        self._current_mac = mac\n",
    "\n",
    "        row = self.df_tuples[self.df_tuples[\"mac\"] == mac]\n",
    "        if not row.empty:\n",
    "            r = row.iloc[0]\n",
    "            self.meta_label.config(text=f\"UUID={r[self.uuid_col]} | Major={r['major']} | Minor={r['minor']}\")\n",
    "        else:\n",
    "            self.meta_label.config(text=\"\")\n",
    "\n",
    "        self.session_table.delete(*self.session_table.get_children())\n",
    "\n",
    "        dfm = self.df_sessions[self.df_sessions[\"mac\"] == mac].copy()\n",
    "        if dfm.empty:\n",
    "            self.evidence.delete(\"1.0\", \"end\")\n",
    "            return\n",
    "\n",
    "        dfm = dfm.sort_values(\"session_start\").reset_index(drop=True)\n",
    "        dfm[\"duration\"] = dfm[\"session_end\"] - dfm[\"session_start\"]\n",
    "        dfm[\"next_gap\"] = dfm[\"session_start\"].shift(-1) - dfm[\"session_end\"]\n",
    "\n",
    "        for _, r in dfm.iterrows():\n",
    "            start_utc = r[\"session_start\"]\n",
    "            end_utc = r[\"session_end\"]\n",
    "            self.session_table.insert(\"\", \"end\", values=(\n",
    "                int(r[\"session_id\"]) if pd.notna(r[\"session_id\"]) else \"\",\n",
    "                start_utc.isoformat(),\n",
    "                end_utc.isoformat(),\n",
    "                self._fmt_tdelta(r[\"duration\"]),\n",
    "                self._fmt_tdelta(r[\"next_gap\"]),\n",
    "                start_utc.isoformat(),\n",
    "                end_utc.isoformat(),\n",
    "            ))\n",
    "\n",
    "        self.evidence.delete(\"1.0\", \"end\")\n",
    "\n",
    "    def on_session_selected(self, event=None):\n",
    "        sel = self.session_table.selection()\n",
    "        if not sel:\n",
    "            return\n",
    "\n",
    "        values = self.session_table.item(sel[0])[\"values\"]\n",
    "        if len(values) < 7:\n",
    "            return\n",
    "\n",
    "        start_utc = pd.to_datetime(values[5], utc=True, errors=\"coerce\")\n",
    "        end_utc = pd.to_datetime(values[6], utc=True, errors=\"coerce\")\n",
    "        if pd.isna(start_utc) or pd.isna(end_utc):\n",
    "            return\n",
    "\n",
    "        mac = self._current_mac\n",
    "        self.evidence.delete(\"1.0\", \"end\")\n",
    "\n",
    "        mask = (\n",
    "            self.df_raw[\"ts_utc\"].between(start_utc, end_utc, inclusive=\"both\") &\n",
    "            self.df_raw[\"mac_list\"].apply(lambda xs: mac in (xs or []))\n",
    "        )\n",
    "\n",
    "        for _, r in self.df_raw.loc[mask].sort_values(\"ts_utc\").iterrows():\n",
    "            ev = self.extract_events_for_mac(r[\"revelations_clean\"], mac)\n",
    "            if ev:\n",
    "                self.evidence.insert(\"end\", f\"- {r['ts_utc'].isoformat()}\\n{ev}\\n\\n\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Tab 4: Track Devices\n",
    "    # ---------------------------------------------------------\n",
    "    def _build_tab_track_devices(self, parent):\n",
    "        top = ttk.Frame(parent)\n",
    "        top.pack(fill=\"x\", padx=10, pady=6)\n",
    "\n",
    "        ttk.Label(top, text=\"Beacon MAC:\").pack(side=\"left\")\n",
    "        self.track_mac_var = tk.StringVar()\n",
    "\n",
    "        self.track_mac_combo = ttk.Combobox(\n",
    "            top, values=self._list_macs, textvariable=self.track_mac_var, state=\"readonly\", width=30\n",
    "        )\n",
    "        self.track_mac_combo.pack(side=\"left\", padx=6)\n",
    "        self.track_mac_combo.bind(\"<<ComboboxSelected>>\", self.on_track_mac_selected)\n",
    "\n",
    "        self.track_meta_label = ttk.Label(top, text=\"\", font=(\"Courier\", 10))\n",
    "        self.track_meta_label.pack(side=\"left\", padx=18)\n",
    "\n",
    "        paned = ttk.Panedwindow(parent, orient=\"horizontal\")\n",
    "        paned.pack(fill=\"both\", expand=True, padx=10, pady=6)\n",
    "\n",
    "        left = ttk.Frame(paned)\n",
    "        right = ttk.Frame(paned)\n",
    "        paned.add(left, weight=4)\n",
    "        paned.add(right, weight=2)\n",
    "\n",
    "        left_box = ttk.LabelFrame(left, text=\"Timestamp → RSSI (dBm) → Proximity (NEAR/FAR/MID)\")\n",
    "        left_box.pack(fill=\"both\", expand=True)\n",
    "\n",
    "        self.track_table = ttk.Treeview(\n",
    "            left_box, columns=(\"ts\", \"rssi\", \"prox\"),\n",
    "            show=\"headings\", height=18\n",
    "        )\n",
    "        self.track_table.heading(\"ts\", text=\"timestamp \")\n",
    "        self.track_table.heading(\"rssi\", text=\"rssi (dBm)\")\n",
    "        self.track_table.heading(\"prox\", text=\"proximity class\")\n",
    "\n",
    "        self.track_table.column(\"ts\", anchor=\"w\", width=520, stretch=True)\n",
    "        self.track_table.column(\"rssi\", anchor=\"center\", width=90, stretch=False)\n",
    "        self.track_table.column(\"prox\", anchor=\"center\", width=160, stretch=False)\n",
    "\n",
    "        yscroll = ttk.Scrollbar(left_box, orient=\"vertical\", command=self.track_table.yview)\n",
    "        self.track_table.configure(yscrollcommand=yscroll.set)\n",
    "\n",
    "        self.track_table.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "        yscroll.pack(side=\"right\", fill=\"y\")\n",
    "\n",
    "        right_box = ttk.LabelFrame(right, text=\"Decision Summary\")\n",
    "        right_box.pack(fill=\"both\", expand=True)\n",
    "\n",
    "        self.track_summary_text = tk.Text(right_box, wrap=\"word\")\n",
    "        self.track_summary_text.pack(fill=\"both\", expand=True)\n",
    "        self.track_summary_text.insert(\"1.0\", \"Select a MAC to compute decisions.\\n\")\n",
    "        self.track_summary_text.configure(state=\"disabled\")\n",
    "\n",
    "    def on_track_mac_selected(self, event=None):\n",
    "        mac = self.track_mac_var.get().upper().strip()\n",
    "\n",
    "        row = self.df_tuples[self.df_tuples[\"mac\"] == mac]\n",
    "        if not row.empty:\n",
    "            r = row.iloc[0]\n",
    "            self.track_meta_label.config(text=f\"UUID={r[self.uuid_col]} | Major={r['major']} | Minor={r['minor']}\")\n",
    "        else:\n",
    "            self.track_meta_label.config(text=\"\")\n",
    "\n",
    "        self.track_table.delete(*self.track_table.get_children())\n",
    "        self.track_summary_text.configure(state=\"normal\")\n",
    "        self.track_summary_text.delete(\"1.0\", \"end\")\n",
    "        self.track_summary_text.configure(state=\"disabled\")\n",
    "\n",
    "        mask = self.df_raw[\"mac_list\"].apply(lambda xs: mac in (xs or []))\n",
    "        rows = self.df_raw.loc[mask].sort_values(\"ts_utc\")\n",
    "\n",
    "        samples = []\n",
    "        rssi_only = []\n",
    "        prox_counts = {\"NEAR\": 0, \"FAR\": 0, \"MID\": 0}\n",
    "\n",
    "        for _, rr in rows.iterrows():\n",
    "            ev = self.extract_events_for_mac(rr[\"revelations_clean\"], mac)\n",
    "            text_to_parse = ev if ev else rr[\"revelations_clean\"]\n",
    "\n",
    "            vals = self._parse_rssi_from_text(text_to_parse)\n",
    "            for v in vals:\n",
    "                prox = self._proximity_label(v)\n",
    "                if prox is None:\n",
    "                    continue\n",
    "                samples.append((rr[\"ts_utc\"], v, prox))\n",
    "                rssi_only.append(v)\n",
    "                prox_counts[prox] += 1\n",
    "\n",
    "        for ts, v, prox in samples:\n",
    "            self.track_table.insert(\"\", \"end\", values=(ts.isoformat(), int(v), prox))\n",
    "        total_prox = sum(prox_counts.values())\n",
    "        prox_major = max(prox_counts.items(), key=lambda kv: kv[1])[0] if total_prox else \"MID\"\n",
    "\n",
    "        motion_label, std, median_step, n = self._detect_moving_or_static_mad(rssi_only)\n",
    "\n",
    "        lines = []\n",
    "        lines.append(f\"MAC: {mac}\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"1) Proximity Status (NEAR / FAR / MID)\")\n",
    "        lines.append(\"   Thresholds (RSSI in dBm):\")\n",
    "        lines.append(f\"     NEAR      if rssi >= {NEAR_RSSI_THRESHOLD} dBm\")\n",
    "        lines.append(f\"     FAR       if rssi <= {FAR_RSSI_THRESHOLD} dBm\")\n",
    "        lines.append(f\"     MID       if {FAR_RSSI_THRESHOLD} < rssi < {NEAR_RSSI_THRESHOLD} dBm\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(f\"   Total RSSI samples: {total_prox}\")\n",
    "        lines.append(\"   Counts:\")\n",
    "        lines.append(f\"     NEAR     : {prox_counts['NEAR']}\")\n",
    "        lines.append(f\"     MID      : {prox_counts['MID']}\")\n",
    "        lines.append(f\"     FAR      : {prox_counts['FAR']}\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"2) Behavior Decision (MOVING / STATIC) [MAD-based]\")\n",
    "        lines.append(\"   Metrics:\")\n",
    "        lines.append(\"     std = 1.4826 * MAD (dB)\")\n",
    "        lines.append(\"     MAD = median(|RSSI - median(RSSI)|)\")\n",
    "        lines.append(\"     step = median(|diff(RSSI)|) (dB)\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"   Thresholds:\")\n",
    "        lines.append(f\"     STATIC if std <= {STATIC_STD_MAX}\")\n",
    "        lines.append(f\"     MOVING if std >= {MOVING_STD_MIN}\")\n",
    "        lines.append(f\"     MID-zone: MOVING if step >= {MOVING_STEP_MIN} else STATIC\")\n",
    "        lines.append(\"\")\n",
    "\n",
    "        if std is None or median_step is None:\n",
    "            lines.append(f\"   Decision: {motion_label} | samples={n} (not enough data for stable stats)\")\n",
    "        else:\n",
    "            lines.append(f\"   std        : {std:.2f} dB\")\n",
    "            lines.append(f\"   step       : {median_step:.2f} dB\")\n",
    "            lines.append(f\"   Decision   : {motion_label}\")\n",
    "\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"Combined:\")\n",
    "        lines.append(f\"  Proximity majority = {prox_major}\")\n",
    "        lines.append(f\"  Movement behavior  = {motion_label}\")\n",
    "\n",
    "        self.track_summary_text.configure(state=\"normal\")\n",
    "        self.track_summary_text.insert(\"1.0\", \"\\n\".join(lines))\n",
    "        self.track_summary_text.configure(state=\"disabled\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Tab 5: Plots\n",
    "    # ---------------------------------------------------------\n",
    "    def _build_tab_analytics(self, parent):\n",
    "        top = ttk.Frame(parent)\n",
    "        top.pack(fill=\"x\", padx=10, pady=(10, 6))\n",
    "\n",
    "        ttk.Label(top, text=\"MAC:\").pack(side=\"left\")\n",
    "        self.analytics_mac_var = tk.StringVar()\n",
    "        self.analytics_mac_combo = ttk.Combobox(\n",
    "            top, values=[], textvariable=self.analytics_mac_var, state=\"readonly\", width=30\n",
    "        )\n",
    "        self.analytics_mac_combo.pack(side=\"left\", padx=6)\n",
    "        self.analytics_mac_combo.bind(\n",
    "            \"<<ComboboxSelected>>\",\n",
    "            lambda e: self._refresh_analytics_for_mac(self.analytics_mac_var.get())\n",
    "        )\n",
    "\n",
    "        ttk.Button(\n",
    "            top, text=\"Refresh\",\n",
    "            command=lambda: self._refresh_analytics_for_mac(self.analytics_mac_var.get())\n",
    "        ).pack(side=\"left\", padx=6)\n",
    "\n",
    "        ttk.Button(\n",
    "            top, text=\"Export Plot (PNG/JPG/PDF)\",\n",
    "            command=self._export_current_plot_image\n",
    "        ).pack(side=\"left\", padx=6)\n",
    "\n",
    "        ttk.Button(\n",
    "            top, text=\"Export Plot Data (CSV/XLSX)\",\n",
    "            command=self._export_current_plot_data\n",
    "        ).pack(side=\"left\", padx=6)\n",
    "\n",
    "        self.analytics_info = ttk.Label(top, text=\"\")\n",
    "        self.analytics_info.pack(side=\"left\", padx=12)\n",
    "\n",
    "        body = ttk.Panedwindow(parent, orient=\"horizontal\")\n",
    "        body.pack(fill=\"both\", expand=True, padx=10, pady=(0, 10))\n",
    "\n",
    "        left = ttk.Frame(body)\n",
    "        right = ttk.Frame(body)\n",
    "        body.add(left, weight=2)\n",
    "        body.add(right, weight=5)\n",
    "\n",
    "        stats_box = ttk.LabelFrame(left, text=\"Per-device stats\")\n",
    "        stats_box.pack(fill=\"both\", expand=True)\n",
    "\n",
    "        self.analytics_stats = tk.Text(stats_box, wrap=\"word\")\n",
    "        self.analytics_stats.pack(fill=\"both\", expand=True)\n",
    "        self.analytics_stats.insert(\"1.0\", \"Select a MAC to view analytics.\\n\")\n",
    "        self.analytics_stats.configure(state=\"disabled\")\n",
    "\n",
    "        self.plot_nb = ttk.Notebook(right)\n",
    "        self.plot_nb.pack(fill=\"both\", expand=True)\n",
    "\n",
    "        self._plot_tabs = {}\n",
    "        for name in [\n",
    "            \"RSSI vs Time\",\n",
    "            \"Proximity vs Time\",\n",
    "            \"Movement Timeline\",\n",
    "            \"RSSI PDF\",\n",
    "            \"RSSI CDF\",\n",
    "            \"Event Count\",\n",
    "        ]:\n",
    "            frame = ttk.Frame(self.plot_nb)\n",
    "            self.plot_nb.add(frame, text=name)\n",
    "            self._plot_tabs[name] = self._make_plot_canvas(frame)\n",
    "\n",
    "        # -------------------- PATCH ADDITION (export tab) --------------------\n",
    "        self.plot_nb.bind(\"<<NotebookTabChanged>>\", self._on_plot_tab_changed)\n",
    "        self._on_plot_tab_changed()\n",
    "        # ---------------------------------------------------------------------------\n",
    "\n",
    "    def _make_plot_canvas(self, parent):\n",
    "        # Bigger size for Tkinter display\n",
    "        fig = Figure(figsize=(IEEE_WIDTH, IEEE_HEIGHT), dpi=300)\n",
    "        ax = fig.add_subplot(111)\n",
    "        canvas = FigureCanvasTkAgg(fig, master=parent)\n",
    "        widget = canvas.get_tk_widget()\n",
    "        widget.pack(fill=\"both\", expand=True)\n",
    "        return {\"fig\": fig, \"ax\": ax, \"canvas\": canvas}\n",
    "\n",
    "\n",
    "    def _clear_ax(self, name: str):\n",
    "        p = self._plot_tabs[name]\n",
    "        p[\"fig\"].clear()\n",
    "        ax = p[\"fig\"].add_subplot(111)\n",
    "        p[\"ax\"] = ax\n",
    "        return ax, p[\"canvas\"], p[\"fig\"]\n",
    "\n",
    "    def _refresh_analytics_for_mac(self, mac: str):\n",
    "        mac = (mac or \"\").upper().strip()\n",
    "        if not mac:\n",
    "            return\n",
    "\n",
    "        df_s, df_bins = self._build_analytics_dataset(mac)\n",
    "        self._compute_common_time_range(df_s, df_bins)\n",
    "\n",
    "        n_rssi = int(df_s.shape[0]) if not df_s.empty else 0\n",
    "        obs = \"\"\n",
    "        if not df_s.empty:\n",
    "            t0 = df_s[\"ts_utc\"].min()\n",
    "            t1 = df_s[\"ts_utc\"].max()\n",
    "            obs = f\" | span={t1 - t0}\"\n",
    "        self.analytics_info.config(text=f\"RSSI samples={n_rssi}{obs}\")\n",
    "\n",
    "        self._render_analytics_stats(mac, df_s, df_bins)\n",
    "        self._plot_rssi_time(mac, df_s)\n",
    "        self._plot_proximity_time(mac, df_s)\n",
    "        self._plot_movement_timeline(mac, df_s)\n",
    "        self._plot_pdf(mac, df_s)\n",
    "        self._plot_cdf(mac, df_s)\n",
    "        self._plot_event_count(mac, df_bins)\n",
    "        self._on_plot_tab_changed()\n",
    "\n",
    "    def _render_analytics_stats(self, mac: str, df_s: pd.DataFrame, df_bins: pd.DataFrame):\n",
    "        lines = []\n",
    "        lines.append(f\"MAC: {mac}\")\n",
    "        lines.append(f\"Event binning: {self.EVENT_BIN_LABEL} | Rolling window: {ROLLING_WINDOW} samples\")\n",
    "        lines.append(\"\")\n",
    "\n",
    "        if df_s.empty:\n",
    "            lines.append(\"No RSSI samples found for this MAC.\")\n",
    "        else:\n",
    "            r = df_s[\"rssi\"].astype(float).values\n",
    "            mean = float(np.mean(r))\n",
    "            med = float(np.median(r))\n",
    "            std = float(np.std(r, ddof=1)) if r.size >= 2 else 0.0\n",
    "            mad_std = float(self._mad_std(r))\n",
    "            p10, p50, p90 = np.percentile(r, [10, 50, 90])\n",
    "\n",
    "            prox_counts = df_s[\"prox\"].value_counts().to_dict()\n",
    "            near = int(prox_counts.get(\"NEAR\", 0))\n",
    "            MID  = int(prox_counts.get(\"MID\", 0))\n",
    "            far  = int(prox_counts.get(\"FAR\", 0))\n",
    "            total = int(r.size)\n",
    "\n",
    "            lines.append(\"RSSI Statistics (dBm):\")\n",
    "            lines.append(f\"  N samples     : {total}\")\n",
    "            lines.append(f\"  mean          : {mean:.2f} dBm\")\n",
    "            lines.append(f\"  median        : {med:.2f} dBm\")\n",
    "            lines.append(f\"  std (classic) : {std:.2f} dB\")\n",
    "            lines.append(f\"  std (MAD)     : {mad_std:.2f} dB\")\n",
    "            lines.append(f\"  p10/p50/p90   : {p10:.1f} / {p50:.1f} / {p90:.1f} dBm\")\n",
    "            lines.append(\"\")\n",
    "            lines.append(\"Proximity breakdown:\")\n",
    "            lines.append(f\"  NEAR      : {near}  ({(near/total*100):.1f}%)\")\n",
    "            lines.append(f\"  MID : {MID}   ({(MID/total*100):.1f}%)\")\n",
    "            lines.append(f\"  FAR       : {far}   ({(far/total*100):.1f}%)\")\n",
    "\n",
    "        if not df_bins.empty and {\"NEW\", \"CHG\", \"DEL\"}.issubset(df_bins.columns):\n",
    "            lines.append(\"\")\n",
    "            lines.append(\"Event Totals (MAC-scoped):\")\n",
    "            lines.append(f\"  NEW={int(df_bins['NEW'].sum())}, CHG={int(df_bins['CHG'].sum())}, DEL={int(df_bins['DEL'].sum())}\")\n",
    "\n",
    "        self.analytics_stats.configure(state=\"normal\")\n",
    "        self.analytics_stats.delete(\"1.0\", \"end\")\n",
    "        self.analytics_stats.insert(\"1.0\", \"\\n\".join(lines))\n",
    "        self.analytics_stats.configure(state=\"disabled\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Plots consistent legend style\n",
    "    # ---------------------------------------------------------\n",
    "    def _apply_time_axis_style(self, ax):\n",
    "        ax.set_xlabel(self.TIME_AXIS_LABEL)\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter(self.TIME_TICK_FMT, tz=UTC_TZ))\n",
    "        for lbl in ax.get_xticklabels():\n",
    "            lbl.set_rotation(30)\n",
    "            lbl.set_ha(\"right\")\n",
    "        ax.tick_params(axis=\"x\", pad=8)\n",
    "\n",
    "    def _apply_legend_style(self, ax):\n",
    "        leg = ax.legend(**self.LEGEND_KW)\n",
    "        if leg is not None:\n",
    "            leg.set_title(\"\")  # keep consistent (no legend titles)\n",
    "        return leg\n",
    "        \n",
    "\n",
    "    def _plot_rssi_time(self, mac: str, df_s: pd.DataFrame):\n",
    "        ax, canvas, _ = self._clear_ax(\"RSSI vs Time\")\n",
    "        ax.set_title(f\"RSSI over time — {mac}\")\n",
    "    \n",
    "        # Empty export schema (prevents reusing previous plot df)\n",
    "        if df_s is None or df_s.empty or \"ts_utc\" not in df_s.columns or \"rssi\" not in df_s.columns:\n",
    "            empty = pd.DataFrame(columns=[\"ts_utc\", \"rssi\"])\n",
    "            self._set_current_plot_context(\"RSSI vs Time\", mac, empty)\n",
    "    \n",
    "            ax.text(0.5, 0.5, \"No RSSI samples.\", ha=\"center\", va=\"center\")\n",
    "            ax.set_xlabel(self.TIME_AXIS_LABEL)\n",
    "            ax.set_ylabel(\"RSSI (dBm)\")\n",
    "            self._apply_universal_axis_style(ax)\n",
    "            ax.figure.tight_layout()\n",
    "            canvas.draw()\n",
    "            return\n",
    "    \n",
    "        d = df_s.sort_values(\"ts_utc\").copy()\n",
    "        ts_utc = pd.to_datetime(d[\"ts_utc\"], utc=True, errors=\"coerce\")\n",
    "        d = d.loc[ts_utc.notna()].copy()\n",
    "        ts_utc = pd.to_datetime(d[\"ts_utc\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "\n",
    "        # Plot (UTC tz-aware)\n",
    "        x_plot = ts_utc\n",
    "        y_plot = d[\"rssi\"].astype(float)\n",
    "        \n",
    "        ax.plot(x_plot, y_plot, marker=\"o\", linestyle=\"none\", markersize=3)\n",
    "        self._apply_time_axis_style(ax)\n",
    "        self._apply_common_time_xlim(ax)\n",
    "        ax.set_ylabel(\"RSSI (dBm)\")\n",
    "        self._apply_universal_axis_style(ax)\n",
    "        \n",
    "        # Export ONLY UTC\n",
    "        df_export = pd.DataFrame({\n",
    "            \"ts_utc\": ts_utc,\n",
    "            \"rssi\": y_plot.values,\n",
    "        })\n",
    "        self._set_current_plot_context(\"RSSI vs Time\", mac, df_export)\n",
    "        ax.figure.tight_layout()\n",
    "        canvas.draw()\n",
    "\n",
    "    \n",
    "    def _plot_proximity_time(self, mac: str, df_s: pd.DataFrame):\n",
    "        ax, canvas, _ = self._clear_ax(\"Proximity vs Time\")\n",
    "        ax.set_title(f\"Proximity over time — {mac}\")\n",
    "    \n",
    "        export_cols = [\"ts_utc\", \"rssi\", \"prox\", \"prox_y\"]\n",
    "    \n",
    "        # ---- guard / empty ----\n",
    "        if (\n",
    "            df_s is None or df_s.empty\n",
    "            or \"ts_utc\" not in df_s.columns\n",
    "            or \"prox\" not in df_s.columns\n",
    "            or \"rssi\" not in df_s.columns\n",
    "        ):\n",
    "            empty = pd.DataFrame(columns=export_cols)\n",
    "            self._set_current_plot_context(\"Proximity vs Time\", mac, empty)\n",
    "    \n",
    "            ax.text(0.5, 0.5, \"No RSSI samples.\", ha=\"center\", va=\"center\")\n",
    "            ax.set_xlabel(self.TIME_AXIS_LABEL)\n",
    "            ax.set_ylabel(\"Proximity class\")\n",
    "            self._apply_universal_axis_style(ax)\n",
    "            ax.figure.tight_layout()\n",
    "            canvas.draw()\n",
    "            return\n",
    "    \n",
    "        # ---- clean ----\n",
    "        d = df_s.sort_values(\"ts_utc\").copy()\n",
    "        d[\"ts_utc\"] = pd.to_datetime(d[\"ts_utc\"], utc=True, errors=\"coerce\")\n",
    "        d[\"rssi\"] = pd.to_numeric(d[\"rssi\"], errors=\"coerce\")\n",
    "        d[\"prox\"] = d[\"prox\"].astype(str).str.strip().str.upper()\n",
    "    \n",
    "        # normalize possible variants\n",
    "        d[\"prox\"] = d[\"prox\"].replace({\"MID RANGE\": \"MID\", \"MID-RANGE\": \"MID\"})\n",
    "    \n",
    "        d = d.dropna(subset=[\"ts_utc\", \"rssi\", \"prox\"]).copy()\n",
    "        if d.empty:\n",
    "            empty = pd.DataFrame(columns=export_cols)\n",
    "            self._set_current_plot_context(\"Proximity vs Time\", mac, empty)\n",
    "    \n",
    "            ax.text(0.5, 0.5, \"No valid RSSI/proximity data.\", ha=\"center\", va=\"center\")\n",
    "            ax.set_xlabel(self.TIME_AXIS_LABEL)\n",
    "            ax.set_ylabel(\"Proximity class\")\n",
    "            self._apply_universal_axis_style(ax)\n",
    "            ax.figure.tight_layout()\n",
    "            canvas.draw()\n",
    "            return\n",
    "    \n",
    "        # ---- map proximity to numeric y (categorical) ----\n",
    "        prox_map = {\"FAR\": 0, \"MID\": 1, \"NEAR\": 2}\n",
    "        d[\"prox_y\"] = d[\"prox\"].map(prox_map)\n",
    "        d = d.dropna(subset=[\"prox_y\"]).copy()\n",
    "        if d.empty:\n",
    "            empty = pd.DataFrame(columns=export_cols)\n",
    "            self._set_current_plot_context(\"Proximity vs Time\", mac, empty)\n",
    "    \n",
    "            ax.text(0.5, 0.5, \"No proximity labels available.\", ha=\"center\", va=\"center\")\n",
    "            ax.set_xlabel(self.TIME_AXIS_LABEL)\n",
    "            # ax.set_ylabel(\"Proximity class\")\n",
    "            self._apply_universal_axis_style(ax)\n",
    "            ax.figure.tight_layout()\n",
    "            canvas.draw()\n",
    "            return\n",
    "    \n",
    "        d[\"prox_y\"] = d[\"prox_y\"].astype(int)\n",
    "    \n",
    "        ts_utc = d[\"ts_utc\"]\n",
    "        x_plot = ts_utc    \n",
    "        # ---- plot proximity points (left axis) ----\n",
    "        ax.plot(x_plot, d[\"prox_y\"], marker=\"o\", linestyle=\"none\", markersize=3)\n",
    "    \n",
    "        self._apply_time_axis_style(ax)\n",
    "        self._apply_common_time_xlim(ax)\n",
    "        # ax.set_ylabel(\"Proximity class\")\n",
    "        ax.set_yticks([0, 1, 2])\n",
    "        ax.set_yticklabels([\"FAR\", \"MID\", \"NEAR\"])\n",
    "        ax.set_ylim(-0.25, 2.25) \n",
    "        self._apply_universal_axis_style(ax)\n",
    "    \n",
    "        # =========================================================\n",
    "        # Right-side axis: RSSI (dBm)\n",
    "        # =========================================================\n",
    "        ax2 = ax.twinx()\n",
    "    \n",
    "        # Plot RSSI points on the right axis \n",
    "        ax2.scatter(x_plot, d[\"rssi\"].astype(float), s=12)\n",
    "    \n",
    "        # Use a stable RSSI range; fall back to data-driven if needed\n",
    "        stable_low, stable_high = -100.0, -40.0\n",
    "        rmin = float(np.nanmin(d[\"rssi\"].values))\n",
    "        rmax = float(np.nanmax(d[\"rssi\"].values))\n",
    "        if np.isfinite(rmin) and np.isfinite(rmax):\n",
    "            # If your data is wildly outside BLE RSSI, adapt; otherwise stable view\n",
    "            if rmin < stable_low or rmax > stable_high:\n",
    "                pad = 2.0\n",
    "                ax2.set_ylim(rmin - pad, rmax + pad)\n",
    "            else:\n",
    "                ax2.set_ylim(stable_low, stable_high)\n",
    "        else:\n",
    "            ax2.set_ylim(stable_low, stable_high)\n",
    "    \n",
    "        ax2.set_ylabel(\"RSSI (dBm)\")\n",
    "        ax2.tick_params(axis=\"y\", labelsize=self.PLOT_STYLE[\"tick_size\"])\n",
    "        ax2.yaxis.label.set_fontsize(self.PLOT_STYLE[\"label_size\"])\n",
    "        ax2.tick_params(axis=\"both\", which=\"major\", labelsize=self.PLOT_STYLE[\"tick_size\"])\n",
    "        ax2.tick_params(axis=\"both\", which=\"minor\", labelsize=self.PLOT_STYLE[\"tick_size\"])\n",
    "        \n",
    "    \n",
    "        # show threshold lines on RSSI axis for visual validation\n",
    "        # (NEAR >= -50, FAR <= -75)\n",
    "        try:\n",
    "            ax2.axhline(NEAR_RSSI_THRESHOLD, linewidth=1.4, alpha=0.35)\n",
    "            ax2.axhline(FAR_RSSI_THRESHOLD, linewidth=1.4, alpha=0.35)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "        # ---- export ----\n",
    "        df_export = pd.DataFrame({\n",
    "            \"ts_utc\": ts_utc,\n",
    "            \"rssi\": d[\"rssi\"].astype(float).values,\n",
    "            \"prox\": d[\"prox\"].astype(str).values,\n",
    "            \"prox_y\": d[\"prox_y\"].astype(int).values,\n",
    "        })\n",
    "        self._set_current_plot_context(\"Proximity vs Time\", mac, df_export)\n",
    "        ax.figure.tight_layout()    \n",
    "        canvas.draw()\n",
    "\n",
    "    def _plot_movement_timeline(self, mac: str, df_s: pd.DataFrame):\n",
    "        ax, canvas, fig = self._clear_ax(\"Movement Timeline\")\n",
    "        ax.set_title(f\"Movement indicator over time — {mac}\")\n",
    "    \n",
    "        export_cols = [\"ts_utc\", \"roll_mad_std\", \"roll_step\", \"label\", \"is_moving\"]\n",
    "    \n",
    "        # -------------------- guards --------------------\n",
    "        if df_s is None or df_s.empty or \"ts_utc\" not in df_s.columns or \"rssi\" not in df_s.columns:\n",
    "            empty = pd.DataFrame(columns=export_cols)\n",
    "            self._set_current_plot_context(\"Movement Timeline\", mac, empty)\n",
    "    \n",
    "            ax.text(0.5, 0.5, \"No RSSI samples.\", ha=\"center\", va=\"center\")\n",
    "            ax.set_ylabel(\"MAD-std (dB)\")\n",
    "    \n",
    "            # align time axis style with other plots\n",
    "            self._apply_time_axis_style(ax)\n",
    "            self._apply_common_time_xlim(ax)\n",
    "    \n",
    "            self._apply_universal_axis_style(ax)\n",
    "            ax.figure.tight_layout()\n",
    "            canvas.draw()\n",
    "            return\n",
    "    \n",
    "        d = df_s.sort_values(\"ts_utc\").copy()\n",
    "        ts_utc = pd.to_datetime(d[\"ts_utc\"], utc=True, errors=\"coerce\")\n",
    "        d = d.loc[ts_utc.notna()].copy()\n",
    "        ts_utc = pd.to_datetime(d[\"ts_utc\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "        if d[\"rssi\"].dropna().empty:\n",
    "            empty = pd.DataFrame(columns=export_cols)\n",
    "            self._set_current_plot_context(\"Movement Timeline\", mac, empty)\n",
    "        \n",
    "            ax.text(0.5, 0.5, \"No RSSI samples.\", ha=\"center\", va=\"center\")\n",
    "            ax.set_ylabel(\"MAD-std (dB)\")\n",
    "        \n",
    "            # align time axis style with other plots\n",
    "            self._apply_time_axis_style(ax)\n",
    "            self._apply_common_time_xlim(ax)\n",
    "        \n",
    "            self._apply_universal_axis_style(ax)\n",
    "            ax.figure.tight_layout()\n",
    "            canvas.draw()\n",
    "            return\n",
    "    \n",
    "        # -------------------- rolling stats --------------------\n",
    "        s = pd.Series(d[\"rssi\"].astype(float).values, index=ts_utc)\n",
    "        mov = self._rolling_movement(s, window=ROLLING_WINDOW)\n",
    "    \n",
    "        if mov.empty or mov[\"label\"].dropna().empty:\n",
    "            empty = pd.DataFrame(columns=export_cols)\n",
    "            self._set_current_plot_context(\"Movement Timeline\", mac, empty)\n",
    "    \n",
    "            ax.text(0.5, 0.5, f\"Not enough data for rolling window={ROLLING_WINDOW}.\",\n",
    "                    ha=\"center\", va=\"center\")\n",
    "            ax.set_ylabel(\"MAD-std (dB)\")\n",
    "    \n",
    "            # align time axis style with other plots\n",
    "            self._apply_time_axis_style(ax)\n",
    "            self._apply_common_time_xlim(ax)\n",
    "    \n",
    "            self._apply_universal_axis_style(ax)\n",
    "            ax.figure.tight_layout()\n",
    "            canvas.draw()\n",
    "            return\n",
    "\n",
    "        def _med_step(x):\n",
    "            x = np.array(x, dtype=float)\n",
    "            if x.size < 2:\n",
    "                return 0.0\n",
    "            return float(np.median(np.abs(np.diff(x))))\n",
    "    \n",
    "        roll_step = s.rolling(ROLLING_WINDOW).apply(_med_step, raw=False).reindex(mov.index)\n",
    "    \n",
    "        # -------------------- plotting --------------------\n",
    "        mov_plot = mov.copy()\n",
    "        mov_plot = mov_plot[~mov_plot.index.duplicated(keep=\"last\")]\n",
    "    \n",
    "        ts_idx_plot = pd.DatetimeIndex(mov_plot.index)\n",
    "        x_plot = ts_idx_plot\n",
    "    \n",
    "        ax.step(\n",
    "            x_plot,\n",
    "            mov_plot[\"roll_mad_std\"].astype(float),\n",
    "            where=\"post\",\n",
    "            label=\"RSSI spread (MAD-based)\",\n",
    "            linewidth=1.4,\n",
    "        )\n",
    "    \n",
    "        ax.axhline(STATIC_STD_MAX, linestyle=\"--\", linewidth=1.0, alpha=0.6,\n",
    "                   label=f\"STATIC ≤ {STATIC_STD_MAX:g} dB\")\n",
    "        ax.axhline(MOVING_STD_MIN, linestyle=\"--\", linewidth=1.0, alpha=0.6,\n",
    "                   label=f\"MOVING ≥ {MOVING_STD_MIN:g} dB\")\n",
    "    \n",
    "        labels_plot = mov_plot[\"label\"].fillna(\"STATIC\").astype(str).values\n",
    "        moving_mask = np.array([1 if v == \"MOVING\" else 0 for v in labels_plot], dtype=int)\n",
    "    \n",
    "        first_span = True\n",
    "        in_seg = False\n",
    "        start = 0\n",
    "        for i, flag in enumerate(moving_mask):\n",
    "            if flag == 1 and not in_seg:\n",
    "                in_seg = True\n",
    "                start = i\n",
    "            if in_seg and (flag == 0 or i == len(moving_mask) - 1):\n",
    "                end = (i - 1) if flag == 0 else i\n",
    "                if end > start:\n",
    "                    ax.axvspan(\n",
    "                        x_plot[start],\n",
    "                        x_plot[end],\n",
    "                        alpha=0.20,\n",
    "                        zorder=0,\n",
    "                        label=\"MOVING interval\" if first_span else None,\n",
    "                    )\n",
    "                    first_span = False\n",
    "                in_seg = False\n",
    "    \n",
    "        y = mov_plot[\"roll_mad_std\"].astype(float).values\n",
    "        if np.isfinite(y).any():\n",
    "            ymin = float(np.nanmin(y))\n",
    "            ymax = float(np.nanmax(y))\n",
    "            ax.set_ylim(max(0.0, ymin - 0.2), ymax + 0.2)\n",
    "    \n",
    "        # -------------------- styling --------------------\n",
    "        ax.set_ylabel(\"MAD-std (dB)\")\n",
    "        ax.grid(False)\n",
    "    \n",
    "        self._apply_time_axis_style(ax)\n",
    "        self._apply_common_time_xlim(ax)\n",
    "        self._apply_legend_style(ax)\n",
    "    \n",
    "        self._apply_universal_axis_style(ax)\n",
    "    \n",
    "        # -------------------- export --------------------\n",
    "        ts_idx = pd.DatetimeIndex(mov.index)\n",
    "        df_export = pd.DataFrame({\n",
    "            \"ts_utc\": ts_idx,\n",
    "            \"roll_mad_std\": mov[\"roll_mad_std\"].astype(float).values,\n",
    "            \"roll_step\": roll_step.astype(float).values,\n",
    "            \"label\": mov[\"label\"].fillna(\"STATIC\").astype(str).values,\n",
    "            \"is_moving\": (mov[\"label\"].fillna(\"STATIC\").astype(str).values == \"MOVING\"),\n",
    "        })\n",
    "        self._set_current_plot_context(\"Movement Timeline\", mac, df_export)\n",
    "        ax.figure.tight_layout()\n",
    "        canvas.draw()\n",
    "\n",
    "\n",
    "    def _plot_pdf(self, mac: str, df_s: pd.DataFrame):\n",
    "        ax, canvas, _ = self._clear_ax(\"RSSI PDF\")\n",
    "        ax.set_title(f\"RSSI distribution (PDF) — {mac}\")\n",
    "    \n",
    "        export_cols = [\n",
    "            \"hist_bin_left\", \"hist_bin_right\", \"hist_density\",\n",
    "            \"fit_x\", \"fit_pdf\", \"mu_dBm\", \"sigma_dB\"\n",
    "        ]\n",
    "    \n",
    "        if df_s is None or df_s.empty or \"rssi\" not in df_s.columns:\n",
    "            empty = pd.DataFrame(columns=export_cols)\n",
    "            self._set_current_plot_context(\"RSSI PDF\", mac, empty)\n",
    "    \n",
    "            ax.text(0.5, 0.5, \"No RSSI samples.\", ha=\"center\", va=\"center\")\n",
    "            ax.set_xlabel(\"RSSI (dBm)\")\n",
    "            ax.set_ylabel(\"Probability density (1/dBm)\")\n",
    "            self._apply_universal_axis_style(ax)\n",
    "            ax.figure.tight_layout()\n",
    "            canvas.draw()\n",
    "            return\n",
    "    \n",
    "        r = df_s[\"rssi\"].astype(float).dropna().values\n",
    "        if r.size == 0:\n",
    "            empty = pd.DataFrame(columns=export_cols)\n",
    "            self._set_current_plot_context(\"RSSI PDF\", mac, empty)\n",
    "    \n",
    "            ax.text(0.5, 0.5, \"No RSSI samples.\", ha=\"center\", va=\"center\")\n",
    "            ax.set_xlabel(\"RSSI (dBm)\")\n",
    "            ax.set_ylabel(\"Probability density (1/dBm)\")\n",
    "            self._apply_universal_axis_style(ax)\n",
    "            ax.figure.tight_layout()\n",
    "            canvas.draw()\n",
    "            return\n",
    "    \n",
    "        # -------- Histogram --------\n",
    "        bins = 25\n",
    "        hist_density, bin_edges = np.histogram(r, bins=bins, density=True)\n",
    "        bin_left = bin_edges[:-1]\n",
    "        bin_right = bin_edges[1:]\n",
    "    \n",
    "        ax.hist(\n",
    "            r,\n",
    "            bins=bins,\n",
    "            density=True,\n",
    "            alpha=0.65,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=2.8\n",
    "        )\n",
    "    \n",
    "        # -------- Normal Fit --------\n",
    "        mu = float(np.mean(r))\n",
    "        sigma = float(np.std(r, ddof=1)) if r.size >= 2 else 0.0\n",
    "    \n",
    "        fit_x = np.linspace(r.min(), r.max(), 300)\n",
    "        fit_pdf = np.full_like(fit_x, np.nan, dtype=float)\n",
    "    \n",
    "        if sigma > 0:\n",
    "            fit_pdf = (\n",
    "                1.0 / (sigma * np.sqrt(2 * np.pi))\n",
    "            ) * np.exp(-0.5 * ((fit_x - mu) / sigma) ** 2)\n",
    "    \n",
    "            ax.plot(\n",
    "                fit_x,\n",
    "                fit_pdf,\n",
    "                linewidth=1.4,\n",
    "                label=f\"Normal fit (μ={mu:.1f} dBm, σ={sigma:.1f} dB)\"\n",
    "            )\n",
    "    \n",
    "            self._apply_legend_style(ax)\n",
    "    \n",
    "        # -------- Styling --------\n",
    "        ax.set_xlabel(\"RSSI (dBm)\")\n",
    "        ax.set_ylabel(\"Probability density (1/dBm)\")\n",
    "        ax.grid(True, alpha=0.2)\n",
    "        self._apply_universal_axis_style(ax)\n",
    "    \n",
    "        # -------- Export --------\n",
    "        n_hist = hist_density.size\n",
    "        n_fit = fit_x.size\n",
    "        n = max(n_hist, n_fit)\n",
    "    \n",
    "        def _pad(a, n):\n",
    "            out = np.full(n, np.nan, dtype=float)\n",
    "            out[:len(a)] = a\n",
    "            return out\n",
    "    \n",
    "        df_export = pd.DataFrame({\n",
    "            \"hist_bin_left\": _pad(bin_left, n),\n",
    "            \"hist_bin_right\": _pad(bin_right, n),\n",
    "            \"hist_density\": _pad(hist_density, n),\n",
    "            \"fit_x\": _pad(fit_x, n),\n",
    "            \"fit_pdf\": _pad(fit_pdf, n),\n",
    "            \"mu_dBm\": np.full(n, mu, dtype=float),\n",
    "            \"sigma_dB\": np.full(n, sigma, dtype=float),\n",
    "        })\n",
    "    \n",
    "        self._set_current_plot_context(\"RSSI PDF\", mac, df_export)\n",
    "        ax.figure.tight_layout()\n",
    "        canvas.draw()\n",
    "\n",
    "\n",
    "    def _plot_cdf(self, mac: str, df_s: pd.DataFrame):\n",
    "        ax, canvas, _ = self._clear_ax(\"RSSI CDF\")\n",
    "        ax.set_title(f\"RSSI cumulative distribution (CDF) — {mac}\")\n",
    "    \n",
    "        export_cols = [\"rssi_sorted_dBm\", \"cdf\"]\n",
    "    \n",
    "        if df_s is None or df_s.empty or \"rssi\" not in df_s.columns:\n",
    "            empty = pd.DataFrame(columns=export_cols)\n",
    "            self._set_current_plot_context(\"RSSI CDF\", mac, empty)\n",
    "    \n",
    "            ax.text(0.5, 0.5, \"No RSSI samples.\", ha=\"center\", va=\"center\")\n",
    "            ax.set_xlabel(\"RSSI (dBm)\")\n",
    "            ax.set_ylabel(\"CDF\")\n",
    "            self._apply_universal_axis_style(ax)\n",
    "            ax.figure.tight_layout()\n",
    "            canvas.draw()\n",
    "            return\n",
    "    \n",
    "        r = df_s[\"rssi\"].astype(float).dropna().values\n",
    "        if r.size == 0:\n",
    "            empty = pd.DataFrame(columns=export_cols)\n",
    "            self._set_current_plot_context(\"RSSI CDF\", mac, empty)\n",
    "    \n",
    "            ax.text(0.5, 0.5, \"No RSSI samples.\", ha=\"center\", va=\"center\")\n",
    "            ax.set_xlabel(\"RSSI (dBm)\")\n",
    "            ax.set_ylabel(\"CDF\")\n",
    "            self._apply_universal_axis_style(ax)\n",
    "            ax.figure.tight_layout()\n",
    "            canvas.draw()\n",
    "            return\n",
    "    \n",
    "        # --- CDF computation ---\n",
    "        r_sorted = np.sort(r)\n",
    "        y = np.arange(1, r_sorted.size + 1) / float(r_sorted.size)\n",
    "    \n",
    "        # --- Use step plot ---\n",
    "        ax.step(r_sorted, y, where=\"post\", linewidth=1.4)\n",
    "    \n",
    "        # --- Clean axis limits ---\n",
    "        ax.set_ylim(0, 1.02)\n",
    "        ax.set_xlim(r_sorted.min(), r_sorted.max())\n",
    "    \n",
    "        ax.set_xlabel(\"RSSI (dBm)\")\n",
    "        ax.set_ylabel(\"CDF\")\n",
    "        ax.grid(False)\n",
    "    \n",
    "        self._apply_universal_axis_style(ax)\n",
    "    \n",
    "        df_export = pd.DataFrame({\n",
    "            \"rssi_sorted_dBm\": r_sorted,\n",
    "            \"cdf\": y,\n",
    "        })\n",
    "        self._set_current_plot_context(\"RSSI CDF\", mac, df_export)\n",
    "        ax.figure.tight_layout()\n",
    "        canvas.draw()\n",
    "\n",
    "\n",
    "    def _plot_event_count(self, mac: str, df_bins: pd.DataFrame):\n",
    "        ax, canvas, _ = self._clear_ax(\"Event Count\")\n",
    "        ax.set_title(f\"Advertisement event count (Δt = {self.EVENT_BIN_LABEL}) — {mac}\")\n",
    "    \n",
    "        export_cols = [\"ts_utc\", \"NEW\", \"CHG\", \"DEL\"]\n",
    "    \n",
    "        if (\n",
    "            df_bins is None or df_bins.empty\n",
    "            or \"ts_utc\" not in df_bins.columns\n",
    "            or not {\"NEW\", \"CHG\", \"DEL\"}.issubset(df_bins.columns)\n",
    "        ):\n",
    "            empty = pd.DataFrame(columns=export_cols)\n",
    "            self._set_current_plot_context(\"Event Count\", mac, empty)\n",
    "    \n",
    "            ax.text(0.5, 0.5, \"No MAC-scoped NEW/CHG/DEL events found.\", ha=\"center\", va=\"center\")\n",
    "            ax.set_xlabel(self.TIME_AXIS_LABEL)\n",
    "            ax.set_ylabel(f\"Events / {self.EVENT_BIN_LABEL}\")\n",
    "            self._apply_universal_axis_style(ax)\n",
    "            ax.figure.tight_layout()\n",
    "            canvas.draw()\n",
    "            return\n",
    "    \n",
    "        d = df_bins.sort_values(\"ts_utc\").copy()\n",
    "        ts_utc = pd.to_datetime(d[\"ts_utc\"], utc=True, errors=\"coerce\")\n",
    "        d = d.loc[ts_utc.notna()].copy()\n",
    "        ts_utc = pd.to_datetime(d[\"ts_utc\"], utc=True, errors=\"coerce\")\n",
    "    \n",
    "        x_plot = ts_utc     \n",
    "        ax.plot(x_plot, d[\"NEW\"].astype(float), label=\"NEW\", linewidth=1.4)\n",
    "        ax.plot(x_plot, d[\"CHG\"].astype(float), label=\"CHG\", linewidth=1.4)\n",
    "        ax.plot(x_plot, d[\"DEL\"].astype(float), label=\"DEL\", linewidth=1.4)\n",
    "    \n",
    "        self._apply_time_axis_style(ax)\n",
    "        self._apply_common_time_xlim(ax)\n",
    "        ax.set_ylabel(f\"Events / {self.EVENT_BIN_LABEL}\")\n",
    "        self._apply_legend_style(ax)\n",
    "        self._apply_universal_axis_style(ax)\n",
    "    \n",
    "        df_export = pd.DataFrame({\n",
    "            \"ts_utc\": ts_utc,\n",
    "            \"NEW\": d[\"NEW\"].astype(int).values,\n",
    "            \"CHG\": d[\"CHG\"].astype(int).values,\n",
    "            \"DEL\": d[\"DEL\"].astype(int).values,\n",
    "        })\n",
    "        self._set_current_plot_context(\"Event Count\", mac, df_export)\n",
    "        ax.figure.tight_layout()\n",
    "        canvas.draw()\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Refresh all tabs when filter changes\n",
    "    # ---------------------------------------------------------\n",
    "    def _refresh_all_tabs(self):\n",
    "        macs = self._filtered_macs()\n",
    "    \n",
    "        self.filter_status.config(\n",
    "            text=f\"Active filter: {self._filter_text!r}  | matches: {len(macs)}\"\n",
    "            if self._filter_text else f\"(no filter) | MACs: {len(macs)}\"\n",
    "        )\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # IMPORTANT: If no MACs match filter → reset everything\n",
    "        # ---------------------------------------------------------\n",
    "        if not macs:\n",
    "            # --- Clear iBeacon tab ---\n",
    "            self.iBeacon_text.configure(state=\"normal\")\n",
    "            self.iBeacon_text.delete(\"1.0\", \"end\")\n",
    "            self.iBeacon_text.insert(\"1.0\", \"No MACs match the current filter.\\n\")\n",
    "            self.iBeacon_text.configure(state=\"disabled\")\n",
    "            self.inv_table.delete(*self.inv_table.get_children())\n",
    "    \n",
    "            # --- Clear Presence tab ---\n",
    "            self.pres_table.delete(*self.pres_table.get_children())\n",
    "    \n",
    "            # --- Clear Session Inspector ---\n",
    "            self.mac_var.set(\"\")\n",
    "            self.session_table.delete(*self.session_table.get_children())\n",
    "            self.evidence.delete(\"1.0\", \"end\")\n",
    "            self.meta_label.config(text=\"\")\n",
    "    \n",
    "            # --- Clear Track Devices ---\n",
    "            self.track_mac_var.set(\"\")\n",
    "            self.track_table.delete(*self.track_table.get_children())\n",
    "            self.track_summary_text.configure(state=\"normal\")\n",
    "            self.track_summary_text.delete(\"1.0\", \"end\")\n",
    "            self.track_summary_text.insert(\"1.0\", \"No MACs match the current filter.\\n\")\n",
    "            self.track_summary_text.configure(state=\"disabled\")\n",
    "    \n",
    "            # --- Clear Analytics ---\n",
    "            self.analytics_mac_var.set(\"\")\n",
    "            self.analytics_info.config(text=\"\")\n",
    "            self.analytics_stats.configure(state=\"normal\")\n",
    "            self.analytics_stats.delete(\"1.0\", \"end\")\n",
    "            self.analytics_stats.insert(\"1.0\", \"No MAC selected.\\n\")\n",
    "            self.analytics_stats.configure(state=\"disabled\")\n",
    "    \n",
    "            # --- VERY IMPORTANT: Clear export context ---\n",
    "            self._current_plot_name = None\n",
    "            self._current_plot_mac = None\n",
    "            self._current_plot_df = None\n",
    "            self._plot_data.clear()\n",
    "    \n",
    "            return  # stop here\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # Normal refresh (when MACs exist)\n",
    "        # ---------------------------------------------------------\n",
    "        self._refresh_tab_ibeacon(macs)\n",
    "        self._refresh_tab_presence(macs)\n",
    "        self._refresh_tab_inspector(macs)\n",
    "    \n",
    "        # --- Track Devices ---\n",
    "        self.track_mac_combo.configure(values=macs)\n",
    "        cur4 = (self.track_mac_var.get() or \"\").upper().strip()\n",
    "        if cur4 not in macs:\n",
    "            self.track_mac_var.set(macs[0])\n",
    "            cur4 = macs[0]\n",
    "        if cur4:\n",
    "            self.on_track_mac_selected()\n",
    "    \n",
    "        # --- Analytics ---\n",
    "        self.analytics_mac_combo.configure(values=macs)\n",
    "        cur5 = (self.analytics_mac_var.get() or \"\").upper().strip()\n",
    "        if cur5 not in macs:\n",
    "            self.analytics_mac_var.set(macs[0])\n",
    "            cur5 = macs[0]\n",
    "    \n",
    "        if cur5:\n",
    "            self._refresh_analytics_for_mac(cur5)\n",
    "        else:\n",
    "            self.analytics_info.config(text=\"\")\n",
    "            self.analytics_stats.configure(state=\"normal\")\n",
    "            self.analytics_stats.delete(\"1.0\", \"end\")\n",
    "            self.analytics_stats.insert(\"1.0\", \"No MAC selected.\\n\")\n",
    "            self.analytics_stats.configure(state=\"disabled\")\n",
    "\n",
    "app = BeaconDeepViewer(\n",
    "    mac_beacon_df,\n",
    "    df_beacon_session_detail,\n",
    "    df\n",
    ")\n",
    "app.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
